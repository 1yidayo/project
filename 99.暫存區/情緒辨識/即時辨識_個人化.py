import torch
import torch.nn as nn
from torchvision import transforms, models
import cv2
from PIL import Image
import os
import sys
from collections import deque

# --- ä¸»ç¨‹å¼ ---
PROJECT_DIR = r"C:\MicroExpressionProject"
MODEL_DIR = os.path.join(PROJECT_DIR, "models")
DATA_DIR = os.path.join(PROJECT_DIR, "data")
HAAR_CASCADE_PATH = os.path.join(DATA_DIR, "haarcascade_frontalface_default.xml")

# --- å‹•æ…‹è¼‰å…¥æ¨¡å‹ ---
# æç¤ºï¼šè«‹ç¢ºä¿é€™è£¡çš„åŸºç¤æ¨¡å‹æª”åèˆ‡æ‚¨åœ¨ `3_è¨“ç·´é€šç”¨åŸºç¤æ¨¡å‹.py` ä¸­ç”¢ç”Ÿçš„æª”åä¸€è‡´ï¼
BASE_MODEL_FILENAME = "test.pth" # æˆ–è€…æ‚¨ä¹‹å‰ä½¿ç”¨çš„ "test.pth", "test2_best.pth" ç­‰

username = input("è«‹è¼¸å…¥æ‚¨çš„ä½¿ç”¨è€…åç¨± (è‹¥ç„¡å€‹äººæ¨¡å‹å‰‡ç›´æ¥æŒ‰Enterä½¿ç”¨é€šç”¨æ¨¡å‹): ").strip()
PERSONAL_MODEL_PATH = os.path.join(MODEL_DIR, f"{username}_model.pth")
BASE_MODEL_PATH = os.path.join(MODEL_DIR, BASE_MODEL_FILENAME)

MODEL_TO_LOAD = ""
if username and os.path.exists(PERSONAL_MODEL_PATH):
    MODEL_TO_LOAD = PERSONAL_MODEL_PATH
    print(f"âœ… æ‰¾åˆ°ä½¿ç”¨è€… '{username}' çš„å€‹äººåŒ–æ¨¡å‹ï¼Œæ­£åœ¨è¼‰å…¥...")
elif os.path.exists(BASE_MODEL_PATH):
    MODEL_TO_LOAD = BASE_MODEL_PATH
    print(f"âš ï¸ æ‰¾ä¸åˆ° '{username}' çš„å€‹äººåŒ–æ¨¡å‹ï¼Œå°‡ä½¿ç”¨é€šç”¨åŸºç¤æ¨¡å‹ã€‚")
    print(f"ğŸ’¡ å»ºè­°åŸ·è¡Œ `2_å€‹äººåŒ–æ ¡æº–.py` å’Œ `4_åŸ·è¡Œå³æ™‚å¾®èª¿.py` ä»¥ç²å¾—æ›´ä½³é«”é©—ã€‚")
else:
    print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•æ¨¡å‹ï¼({BASE_MODEL_PATH})")
    print("è«‹å…ˆåŸ·è¡Œè¨“ç·´è…³æœ¬ç”¢ç”ŸåŸºç¤æ¨¡å‹ã€‚")
    input("æŒ‰ Enter éµé€€å‡º...")
    sys.exit(1)

# --- è¼‰å…¥ Checkpoint ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
checkpoint = torch.load(MODEL_TO_LOAD, map_location=device)

if "state_dict" in checkpoint: state_dict = checkpoint["state_dict"]
else: state_dict = checkpoint

if "classes" in checkpoint: classes = checkpoint["classes"]
else:
    print("âš ï¸ checkpoint ä¸­æ²’æœ‰ 'classes'ï¼Œè«‹åœ¨ç¨‹å¼ä¸­æ‰‹å‹•æŒ‡å®š classesã€‚")
    classes = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']

print(f"âœ… æ¨¡å‹è¼‰å…¥å®Œæˆ. å¯è¾¨è­˜è¡¨æƒ…: {classes}")

# --- å»ºç«‹æ¨¡å‹éª¨æ¶ä¸¦è¼‰å…¥æ¬Šé‡ ---
model = models.resnet18(pretrained=False)
fc_keys = [k for k in state_dict.keys() if k.startswith("fc.")]
use_sequential_fc = any(k.startswith("fc.1.") for k in fc_keys)
if use_sequential_fc:
    model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, len(classes)))
else:
    model.fc = nn.Linear(model.fc.in_features, len(classes))

model = model.to(device)
try:
    model.load_state_dict(state_dict)
except RuntimeError as e:
    print(f"âš ï¸ è¼‰å…¥æ¨¡å‹æ¬Šé‡æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    print("å˜—è©¦ä»¥ non-strict æ¨¡å¼è¼‰å…¥...")
    model.load_state_dict(state_dict, strict=False)

model.eval()

# --- åœ–ç‰‡é è™•ç† ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# --- åˆå§‹åŒ–è‡‰éƒ¨åµæ¸¬å™¨èˆ‡æ”å½±æ©Ÿ ---
face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)
if face_cascade.empty():
    print("âŒ éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥ Haar Cascade XML æª”æ¡ˆï¼")
    input("æŒ‰ Enter éµé€€å‡º...")
    sys.exit(1)

cap = cv2.VideoCapture(0)
if not cap.isOpened():
    print("âŒ éŒ¯èª¤ï¼šç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿï¼")
    input("æŒ‰ Enter éµé€€å‡º...")
    sys.exit(1)

print("â–¶ æŒ‰ ESC çµæŸï¼Œæ”å½±æ©Ÿå•Ÿå‹•ä¸­...")

SMOOTH_WINDOW = 5
smooth_queue = deque(maxlen=SMOOTH_WINDOW)

# --- å³æ™‚è¾¨è­˜ä¸»è¿´åœˆ ---
with torch.no_grad():
    while True:
        ret, frame = cap.read()
        if not ret: continue
        
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 8)

        # åœ¨ç•«é¢ä¸Šé¡¯ç¤ºç•¶å‰ä½¿ç”¨çš„æ¨¡å‹
        model_name = os.path.basename(MODEL_TO_LOAD)
        cv2.putText(frame, f"Model: {model_name}", (10, frame.shape[0] - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

        for (x, y, w, h) in faces:
            face_crop = frame[y:y+h, x:x+w]
            img = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(img)
            img_tensor = transform(img).unsqueeze(0).to(device)

            outputs = model(img_tensor)
            probs = torch.softmax(outputs, dim=1)[0]

            # å¹³æ»‘è™•ç†
            smooth_queue.append(probs.cpu())
            avg_probs = torch.stack(list(smooth_queue), dim=0).mean(dim=0)
            probs = avg_probs.to(device)

            # å–å¾—å‰ä¸‰å¤§è¡¨æƒ…
            k = min(3, len(classes))
            top_prob, top_idx = torch.topk(probs, k)
            results = [f"{classes[top_idx[i]]}: {top_prob[i]*100:.1f}%" for i in range(k)]

            # ç¹ªè£½çµæœ
            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)
            
            for i, text in enumerate(results):
                cv2.putText(frame, text, (10, 30 + i*25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)

        cv2.imshow("å€‹äººåŒ–è¡¨æƒ…è¾¨è­˜ç³»çµ±", frame)
        if cv2.waitKey(1) & 0xFF == 27: # ESC éµ
            break

# --- é‡‹æ”¾è³‡æº ---
cap.release()
cv2.destroyAllWindows()