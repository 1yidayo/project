import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, models
import os
import sys
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import cv2

# -----------------------------
# è¨­å®š
# -----------------------------
PROJECT_DIR = r"C:\MicroExpressionProject"
DATA_DIR = os.path.join(PROJECT_DIR, "data")
MODEL_DIR = os.path.join(PROJECT_DIR, "models")
CALIBRATION_DIR = os.path.join(DATA_DIR, "calibration_videos")
HAAR_CASCADE_PATH = os.path.join(DATA_DIR, "haarcascade_frontalface_default.xml")

# --- VideoFrameDataset é¡åˆ¥å®šç¾© ---
class VideoFrameDataset(Dataset):
    def __init__(self, video_dir, transform=None):
        self.video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith((".mp4", ".avi"))]
        self.transform = transform
        self.data = []
        self.labels = []

        face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)
        print(f"æ­£åœ¨å¾ {video_dir} è¼‰å…¥å€‹äººæ ¡æº–å½±ç‰‡...")
        for vid_path in self.video_files:
            # æˆ‘å€‘æ˜ç¢ºçŸ¥é“é€™æ®µæ ¡æº–å½±ç‰‡çš„æ¨™ç±¤æ˜¯ "relaxed"ã€‚
            label_name = "relaxed" 
            
            cap = cv2.VideoCapture(vid_path)
            while True:
                ret, frame = cap.read()
                if not ret: break
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)
                if len(faces) == 0: continue
                x, y, w, h = faces[0]
                face_crop = frame[y:y+h, x:x+w]
                self.data.append(face_crop)
                self.labels.append(label_name)
            cap.release()
        print("å€‹äººæ ¡æº–å½±ç‰‡è¼‰å…¥å®Œæˆï¼")
        
        self.classes = sorted(list(set(self.labels)))
        self.class_to_idx = {cls_name:i for i, cls_name in enumerate(self.classes)}
        self.idx_labels = [self.class_to_idx[l] for l in self.labels]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = self.data[idx]
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(img)
        if self.transform:
            img = self.transform(img)
        label = self.idx_labels[idx]
        return img, label

# --- å¾®èª¿è¶…åƒæ•¸ ---
FINE_TUNE_EPOCHS = 12
FINE_TUNE_LR = 1e-5

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# <<< ä¿®æ”¹ï¼šå¾å‘½ä»¤åˆ—æ¥æ”¶åƒæ•¸ï¼Œæ”¹æˆäº’å‹•å¼è¼¸å…¥ >>>
# --- ä¸»ç¨‹å¼ ---
# 1. è‡ªå‹•æƒæä¸¦åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ä½¿ç”¨è€…
try:
    available_users = [name for name in os.listdir(CALIBRATION_DIR) if os.path.isdir(os.path.join(CALIBRATION_DIR, name))]
except FileNotFoundError:
    print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ ¡æº–è³‡æ–™å¤¾ {CALIBRATION_DIR}")
    input("æŒ‰ Enter éµé€€å‡º...")
    exit()

if not available_users:
    print(f"éŒ¯èª¤ï¼šåœ¨ '{CALIBRATION_DIR}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•ä½¿ç”¨è€…çš„æ ¡æº–è³‡æ–™å¤¾ï¼")
    print("è«‹å…ˆåŸ·è¡Œ '2_å€‹äººåŒ–æ ¡æº–.py' ä¾†å»ºç«‹è³‡æ–™ã€‚")
    input("æŒ‰ Enter éµé€€å‡º...")
    exit()

print("âœ… æ‰¾åˆ°ä»¥ä¸‹å¯ç”¨çš„å€‹äººåŒ–æ ¡æº–è³‡æ–™ï¼š")
for user in available_users:
    print(f"  - {user}")

# 2. è®“ä½¿ç”¨è€…è¼¸å…¥è¦è™•ç†çš„åç¨±ï¼Œä¸¦é©—è­‰
username = ""
while True:
    username = input("\n> è«‹è¼¸å…¥æ‚¨è¦é€²è¡Œå¾®èª¿çš„ä½¿ç”¨è€…åç¨±: ").strip()
    if username in available_users:
        break
    else:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä½¿ç”¨è€… '{username}'ã€‚è«‹å¾ä»¥ä¸Šåˆ—è¡¨ä¸­é¸æ“‡ä¸€å€‹æ­£ç¢ºçš„åç¨±ã€‚")

# (åŸæœ¬æ¥æ”¶ sys.argv çš„éƒ¨åˆ†å·²è¢«ä¸Šé¢å–ä»£)
# â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²

user_video_dir = os.path.join(CALIBRATION_DIR, username)

# --- è¼‰å…¥é€šç”¨åŸºç¤æ¨¡å‹ ---
BASE_MODEL_PATH = os.path.join(MODEL_DIR, "test.pth")
if not os.path.exists(BASE_MODEL_PATH):
    print(f"éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°é€šç”¨åŸºç¤æ¨¡å‹ï¼è«‹å…ˆåŸ·è¡Œ `3_è¨“ç·´é€šç”¨åŸºç¤æ¨¡å‹.py`ã€‚")
    input("æŒ‰ Enter éµé€€å‡º...")
    exit()
    
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"æ­£åœ¨å¾ {BASE_MODEL_PATH} è¼‰å…¥é€šç”¨åŸºç¤æ¨¡å‹...")
checkpoint = torch.load(BASE_MODEL_PATH, map_location=device)
classes = checkpoint['classes']
class_to_idx_base = {cls_name: i for i, cls_name in enumerate(classes)}

model = models.resnet18(pretrained=False)
model.fc = nn.Sequential(
    nn.Dropout(0.3),
    nn.Linear(model.fc.in_features, len(classes))
)
model.load_state_dict(checkpoint['state_dict'])
model = model.to(device)
print("âœ… é€šç”¨åŸºç¤æ¨¡å‹è¼‰å…¥æˆåŠŸï¼")

# --- æº–å‚™å€‹äººåŒ–è³‡æ–™ ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

dataset = VideoFrameDataset(user_video_dir, transform=transform)
if len(dataset) == 0:
    print("éŒ¯èª¤ï¼šå€‹äººæ ¡æº–è³‡æ–™å¤¾ä¸­æ²’æœ‰å¯ç”¨çš„è‡‰éƒ¨è³‡æ–™ï¼")
    input("æŒ‰ Enter éµé€€å‡º...")
    exit()

try:
    relaxed_label_idx_in_base = class_to_idx_base["relaxed"]
    dataset.idx_labels = [relaxed_label_idx_in_base] * len(dataset)
except KeyError:
    print("éŒ¯èª¤ï¼šæ‚¨çš„é€šç”¨åŸºç¤æ¨¡å‹ä¸­ä¸åŒ…å« 'relaxed' é€™å€‹é¡åˆ¥ï¼è«‹ç¢ºèªæ¨™ç±¤åç¨±ã€‚")
    input("æŒ‰ Enter éµé€€å‡º...")
    exit()

dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

# --- é–‹å§‹å¾®èª¿ ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=FINE_TUNE_LR)

print(f"\nğŸš€ é–‹å§‹ç‚ºä½¿ç”¨è€… '{username}' é€²è¡Œ {FINE_TUNE_EPOCHS} è¼ªçš„å¿«é€Ÿå¾®èª¿...")
model.train()
for epoch in range(FINE_TUNE_EPOCHS):
    total_loss = 0
    for imgs, labels in dataloader:
        imgs, labels = imgs.to(device), labels.to(device)
        
        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    
    avg_loss = total_loss / len(dataloader)
    print(f"å¾®èª¿ Epoch {epoch+1}/{FINE_TUNE_EPOCHS}, Loss: {avg_loss:.6f}")

# --- å„²å­˜å€‹äººåŒ–æ¨¡å‹ ---
PERSONAL_MODEL_PATH = os.path.join(MODEL_DIR, f"{username}_model.pth")
torch.save({
    "state_dict": model.state_dict(),
    "classes": classes
}, PERSONAL_MODEL_PATH)

print(f"\nğŸ‰ å¾®èª¿å®Œæˆï¼å·²ç‚º '{username}' ç”¢ç”Ÿå°ˆå±¬æ¨¡å‹æ–¼: {PERSONAL_MODEL_PATH}")
input("æŒ‰ Enter éµé€€å‡º...") # ä¿æŒè¦–çª—é–‹å•Ÿï¼Œç›´åˆ°ä½¿ç”¨è€…ç¢ºèª