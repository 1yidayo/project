import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, models
import cv2
import os
from torch.utils.data import Dataset, DataLoader, random_split
from PIL import Image

# -----------------------------
# è¨­å®šè³‡æ–™å¤¾
# -----------------------------
PROJECT_DIR = r"C:\MicroExpressionProject"
VIDEO_DIR = os.path.join(PROJECT_DIR, "data", "test_videos")
MODEL_DIR = os.path.join(PROJECT_DIR, "models")
MODEL_PATH = os.path.join(MODEL_DIR, "test.pth") # æª”ååŠ ä¸Š best
HAAR_CASCADE_PATH = os.path.join(PROJECT_DIR, "data", "haarcascade_frontalface_default.xml")
os.makedirs(VIDEO_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# -----------------------------
# å½±ç‰‡è³‡æ–™é›† Dataset
# -----------------------------
class VideoFrameDataset(Dataset):
    def __init__(self, video_dir, transform=None):
        self.video_files = [os.path.join(video_dir, f) for f in os.listdir(video_dir) if f.endswith((".mp4", ".avi"))]
        self.transform = transform
        self.data = []
        self.labels = []

        face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)
        for vid_path in self.video_files:
            label_name = os.path.basename(vid_path).split("_")[0]
            cap = cv2.VideoCapture(vid_path)
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)
                if len(faces) == 0:
                    continue
                x, y, w, h = faces[0]
                face_crop = frame[y:y+h, x:x+w]
                self.data.append(face_crop)
                self.labels.append(label_name)
            cap.release()

        self.classes = sorted(list(set(self.labels)))
        self.class_to_idx = {cls_name:i for i, cls_name in enumerate(self.classes)}
        self.idx_labels = [self.class_to_idx[l] for l in self.labels]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        img = self.data[idx]
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        img = Image.fromarray(img)
        if self.transform:
            img = self.transform(img)
        label = self.idx_labels[idx]
        return img, label

# -----------------------------
# Transform & Dataset
# -----------------------------
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

dataset = VideoFrameDataset(VIDEO_DIR, transform=transform)

if len(dataset) == 0:
    print("âš ï¸ æ²’æœ‰ä»»ä½•å¯ç”¨çš„è‡‰éƒ¨è³‡æ–™ï¼Œè«‹ç¢ºèª test_videos è£¡æ˜¯å¦æœ‰å½±ç‰‡")
    exit()

# 80% ä½œç‚ºè¨“ç·´é›†ï¼Œ20% ä½œç‚ºé©—è­‰é›†
train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size
train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

# å»ºç«‹å„è‡ªçš„ DataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

print("âœ… é€™æ¬¡å°‡è¨“ç·´çš„è¡¨æƒ…é¡åˆ¥:", dataset.classes)
print(f"âœ… è³‡æ–™é›†åˆ‡åˆ†å®Œæˆ -> è¨“ç·´é›†: {len(train_dataset)} ç­†, é©—è­‰é›†: {len(val_dataset)} ç­†")

# -----------------------------
# æ¨¡å‹
# -----------------------------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = models.resnet18(pretrained=True)
model.fc = nn.Sequential(
    nn.Dropout(0.3),
    nn.Linear(model.fc.in_features, len(dataset.classes))
)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)

# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
# <<< ä¿®æ”¹é»ï¼šç§»é™¤ verbose=True åƒæ•¸ä»¥ç›¸å®¹èˆŠç‰ˆ PyTorch >>>
# â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼â–¼
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.1, patience=1)

# -----------------------------
# è¼‰å…¥èˆŠæ¨¡å‹ç¹¼çºŒè¨“ç·´ï¼ˆæ”¯æ´é¡åˆ¥è®Šå‹•ï¼‰
# -----------------------------
if os.path.exists(MODEL_PATH):
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    state_dict = checkpoint["state_dict"]

    filtered_dict = {k:v for k,v in state_dict.items() if not k.startswith("fc.1.")}
    model.load_state_dict(filtered_dict, strict=False)
    print("âœ… å·²è¼‰å…¥å…ˆå‰è¨“ç·´éçš„æ¨¡å‹æ¬Šé‡ï¼ˆæœ€å¾Œä¸€å±¤éš¨æ–°é¡åˆ¥æ•¸åˆå§‹åŒ–ï¼‰ï¼Œç¹¼çºŒè¨“ç·´ï¼")
else:
    print("ğŸš€ æ²’æœ‰èˆŠæ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´ã€‚")

# -----------------------------
# è¨“ç·´ï¼ˆå« Early Stopping + Ctrl+C æ•æ‰ï¼‰
# -----------------------------
best_val_loss = float("inf")
patience = 2
counter = 0
delta = 1e-5
max_epochs = 20

print(f"\nğŸš€ é–‹å§‹è¨“ç·´ï¼Œå…± {len(dataset.classes)} ç¨®è¡¨æƒ…: {dataset.classes}\n")

try:
    for epoch in range(max_epochs):
        # --- è¨“ç·´éšæ®µ ---
        model.train()
        total_train_loss = 0
        for imgs, labels in train_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
        avg_train_loss = total_train_loss / len(train_loader)

        # --- é©—è­‰éšæ®µ ---
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs, labels = imgs.to(device), labels.to(device)
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                total_val_loss += loss.item()
        avg_val_loss = total_val_loss / len(val_loader)

        print(f"Epoch {epoch+1}/{max_epochs}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}")

        # Early Stopping æª¢æŸ¥åŸºæ–¼ avg_val_loss
        if avg_val_loss < best_val_loss - delta:
            best_val_loss = avg_val_loss
            counter = 0
            torch.save({
                "state_dict": model.state_dict(),
                "classes": dataset.classes
            }, MODEL_PATH)
            print("âœ… é©—è­‰é›† Loss é™ä½ï¼Œå·²ä¿å­˜æœ€ä½³æ¨¡å‹ï¼")
        else:
            counter += 1
            print(f"âš ï¸ é©—è­‰é›† Loss æœªæ”¹å–„ï¼ˆç¬¬ {counter} æ¬¡ï¼‰")
            if counter >= patience:
                print(f"â¹ï¸ é€£çºŒ {patience} æ¬¡æœªæ”¹å–„ï¼Œæå‰åœæ­¢è¨“ç·´ã€‚")
                break
        
        # æ›´æ–°å­¸ç¿’ç‡
        scheduler.step(avg_val_loss)

except KeyboardInterrupt:
    print(f"\nâ¹ï¸ è¨“ç·´è¢«æ‰‹å‹•ä¸­æ–·ã€‚")

print("\nğŸ‰ è¨“ç·´å®Œæˆï¼Œæœ€ä½³æ¨¡å‹å·²å„²å­˜ï¼")
print("å·²ä¿å­˜è¡¨æƒ…é¡åˆ¥:", dataset.classes)