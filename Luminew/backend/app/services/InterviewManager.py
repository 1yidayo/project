# InterviewManager.py
# å°ˆé¡Œç´šé¢è©¦ç®¡ç†å™¨ï¼ˆSTT / LLM / TTS æ•´åˆï¼‰
import threading
from queue import Queue
from app.services.yating_stt import YatingSTT
from app.services.openai_llm import ask_gpt4_1_nano
from app.services.minimax_tts_ws import MinimaxTTSWS  # ä½¿ç”¨ WebSocket ç‰ˆæœ¬
from professor_persona import get_professor_persona

class InterviewManager:
    """
    ç®¡ç†æ•´å ´æ¨¡æ“¬é¢è©¦æµç¨‹ï¼š
    1. æ¥æ”¶å­¸ç”ŸèªéŸ³ â†’ STT
    2. LLM åˆ†æ â†’ å›ç­”æ•™æˆ
    3. Minimax TTS WebSocket â†’ è¿”å›éŸ³è¨Š chunk çµ¦å‰ç«¯
    """
    def __init__(self, professor_type="warm_industry_professor"):
        # å–å¾— persona
        self.professor_persona = get_professor_persona(professor_type)
        self.system_prompt = self.professor_persona.prompt

        # STT
        self.stt = YatingSTT()

        # TTS (WebSocket)
        self.tts = MinimaxTTSWS(
            api_key=None,  # å¯å¾ .env è®€
            default_voice_id=self.professor_persona.voice_id
        )

        # å°è©±æ­·å²
        self.conversation_history = []

        # éŸ³è¨Š queueï¼ˆå‰ç«¯å‚³éŸ³è¨Š chunk é€²ä¾†ï¼‰
        self.audio_queue = Queue()

        # ç‹€æ…‹
        self.interview_running = False

    # --- å•Ÿå‹•é¢è©¦ ---
    def start_interview(self):
        self.interview_running = True
        print(f"ğŸ“ é¢è©¦é–‹å§‹ï¼ˆæ•™æˆ: {self.professor_persona.name}ï¼‰")
        self.stt.start_recording()
        threading.Thread(target=self._process_audio_loop, daemon=True).start()

    # --- åœæ­¢é¢è©¦ ---
    def stop_interview(self):
        self.interview_running = False
        self.stt.stop_recording()
        print("â¹ é¢è©¦çµæŸ")

    # --- å‰ç«¯å‚³å…¥éŸ³è¨Š chunk ---
    def feed_audio_chunk(self, chunk):
        if self.interview_running:
            self.audio_queue.put(chunk)

    # --- å¾ªç’°è™•ç†éŸ³è¨Š STT / LLM / TTS ---
    def _process_audio_loop(self):
        while self.interview_running:
            chunk = self.audio_queue.get()
            if chunk is None:
                continue

            # 1ï¸âƒ£ STT è½‰æ–‡å­—
            text = self.stt.recognize_chunk(chunk)
            if not text:
                continue
            print("[STT] ", text)

            # ä¿å­˜å°è©±æ­·å²
            self.conversation_history.append({"role": "student", "content": text})

            # 2ï¸âƒ£ å‘¼å« LLM
            prompt = self._build_llm_prompt()
            reply = ask_gpt4_1_nano(prompt, professor_type=self.professor_persona.name)
            print("[Professor] ", reply)

            # ä¿å­˜å°è©±æ­·å²
            self.conversation_history.append({"role": "professor", "content": reply})

            # 3ï¸âƒ£ TTS ç”Ÿæˆ â†’ WebSocket å³æ™‚å›å‚³
            self.tts.speak_stream(
                text=reply,
                voice_id=self.professor_persona.voice_id,
                on_audio_chunk=self._send_audio_chunk_to_frontend
            )

    # --- å»ºç«‹ LLM promptï¼ˆåŒ…å«æ­·å²å°è©±ï¼‰ ---
    def _build_llm_prompt(self):
        prompt_text = ""
        for turn in self.conversation_history[-5:]:  # åªå¸¶æœ€è¿‘å¹¾å¥
            role = turn["role"]
            content = turn["content"]
            if role == "student":
                prompt_text += f"å­¸ç”Ÿèªª: {content}\n"
            else:
                prompt_text += f"æ•™æˆèªª: {content}\n"
        prompt_text += "è«‹ä»¥æ•™æˆèº«ä»½å›ç­”ä¸‹ä¸€å¥ã€‚\n"
        return prompt_text

    # --- å›å‚³å‰ç«¯å–®å€‹éŸ³è¨Š chunk ---
    def _send_audio_chunk_to_frontend(self, audio_bytes):
        """
        callback çµ¦å‰ç«¯ï¼Œæ¯æ”¶åˆ°ä¸€å€‹ TTS audio chunk å°±æœƒå‘¼å«
        """
        # TODO: æ”¹æˆ WebSocket æˆ– FastAPI Streaming å›å‚³
        print("[TTS audio chunk] bytes length:", len(audio_bytes))