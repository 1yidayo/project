# emotion_service.py
# æƒ…ç·’åˆ†ææ ¸å¿ƒæœå‹™ - å¾ Flask app.py é·ç§»åˆ° FastAPI æ¶æ§‹

import os
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
from openai import OpenAI
from dotenv import load_dotenv
import traceback
from collections import deque
import uuid
import json

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ---------------------------
# å…¨åŸŸè¨­å®š
# ---------------------------
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
MODEL_PATH = os.path.join(PROJECT_DIR, "models", "test_best_.pth")
VIDEO_STORAGE_DIR = os.path.join(PROJECT_DIR, "static", "videos")
os.makedirs(VIDEO_STORAGE_DIR, exist_ok=True)

# OpenAI Client
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = None

if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)
    print(f"ğŸ”‘ ç›®å‰ç³»çµ±è®€åˆ°çš„ Key å‰äº”ç¢¼: {OPENAI_API_KEY[:10]}...")
    print("âœ… OpenAI Client è¨­å®šæˆåŠŸ")
else:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")

# è¼‰å…¥äººè‡‰è¾¨è­˜å™¨
HAAR_PATH = os.path.join(PROJECT_DIR, "haarcascade_frontalface_default.xml")
if not os.path.exists(HAAR_PATH):
    print(f"âš ï¸ æœ¬åœ°æ‰¾ä¸åˆ° {HAAR_PATH}ï¼Œå˜—è©¦ä½¿ç”¨ OpenCV å…§å»ºè·¯å¾‘...")
    HAAR_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'

print(f"ğŸ“‚ æ­£åœ¨è¼‰å…¥äººè‡‰è¾¨è­˜æª”ï¼š{HAAR_PATH}")
face_cascade = cv2.CascadeClassifier(HAAR_PATH)

if face_cascade.empty():
    print("âŒ åš´é‡éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥äººè‡‰è¾¨è­˜å™¨ (xml æª”æ¡ˆææ¯€æˆ–è·¯å¾‘éŒ¯èª¤)")

# è¼‰å…¥æƒ…ç·’æ¨¡å‹ (ResNet18)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CLASSES = ['confidence', 'nervous', 'passion', 'relaxed']

model = models.resnet18(pretrained=False)
try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    state_dict = checkpoint["state_dict"] if "state_dict" in checkpoint else checkpoint
    
    fc_keys = [k for k in state_dict.keys() if k.startswith("fc.")]
    use_sequential = any(k.startswith("fc.1.") for k in fc_keys)
    
    if use_sequential:
        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, len(CLASSES)))
    else:
        model.fc = nn.Linear(model.fc.in_features, len(CLASSES))
        
    model.load_state_dict(state_dict, strict=False)
    print("âœ… æƒ…ç·’è¾¨è­˜æ¨¡å‹è¼‰å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    model.fc = nn.Linear(model.fc.in_features, len(CLASSES))

model = model.to(device)
model.eval()

# å½±åƒé è™•ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])


def get_video_storage_dir():
    """å–å¾—å½±ç‰‡å„²å­˜ç›®éŒ„"""
    return VIDEO_STORAGE_DIR


async def analyze_video(video_path: str, save_video: bool = True) -> dict:
    """
    åˆ†æå½±ç‰‡æƒ…ç·’
    
    Args:
        video_path: å½±ç‰‡æª”æ¡ˆè·¯å¾‘
        save_video: æ˜¯å¦ä¿ç•™å½±ç‰‡
        
    Returns:
        åˆ†æçµæœ dict
    """
    try:
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return {"error": "Could not open video"}

        timeline_data = []
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0 or fps is None:
            fps = 30
        frame_interval = max(1, int(fps / 3))

        session_history = []
        frame_count = 0
        detected_count = 0
        
        orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"ğŸ¥ åŸå§‹å½±ç‰‡å°ºå¯¸: {orig_w} x {orig_h}")

        smooth_queue = deque(maxlen=5)

        with torch.no_grad():
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                if frame_count % 3 != 0:
                    continue

                found_face_info = None
                rotation_attempts = [None, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE]
                
                for rot_code in rotation_attempts:
                    temp_frame = frame.copy()
                    
                    if rot_code is not None:
                        temp_frame = cv2.rotate(temp_frame, rot_code)
                    
                    target_w = 480
                    h_curr, w_curr, _ = temp_frame.shape
                    scale = target_w / w_curr
                    new_h = int(h_curr * scale)
                    temp_frame = cv2.resize(temp_frame, (target_w, new_h))
                    
                    gray = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2GRAY)
                    faces = face_cascade.detectMultiScale(gray, 1.1, 8)
                    
                    if len(faces) > 0:
                        found_face_info = (temp_frame, faces)
                        break
                
                if found_face_info is None:
                    continue

                detected_count += 1
                correct_frame, faces = found_face_info
                (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])

                if detected_count <= 3:
                    debug_frame = correct_frame.copy()
                    cv2.rectangle(debug_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.imwrite(f"debug_face_server_{detected_count}.jpg", debug_frame)

                face_crop = correct_frame[y:y+h, x:x+w]

                try:
                    img = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
                    img = Image.fromarray(img)
                    img_tensor = transform(img).unsqueeze(0).to(device)

                    outputs = model(img_tensor)
                    probs = torch.softmax(outputs, dim=1)[0]
                    
                    smooth_queue.append(probs.cpu())
                    avg_probs = torch.stack(list(smooth_queue), dim=0).mean(dim=0)

                    current_emotions = {}
                    for i, cls in enumerate(CLASSES):
                        current_emotions[cls] = avg_probs[i].item()
                    
                    session_history.append(current_emotions)

                    if frame_count % frame_interval == 0:
                        timeline_entry = {
                            "t": round(frame_count / fps, 1),
                            "c": int(current_emotions['confidence'] * 100),
                            "n": int(current_emotions['nervous'] * 100),
                            "p": int(current_emotions['passion'] * 100),
                            "r": int(current_emotions['relaxed'] * 100)
                        }
                        timeline_data.append(timeline_entry)

                except Exception as img_err:
                    print(f"âš ï¸ å½±åƒè™•ç†éŒ¯èª¤: {img_err}")
                    pass

        cap.release()
        print(f"ğŸ“Š åˆ†æå®Œæˆï¼šå…±è®€å– {frame_count} å¹€ï¼ŒæˆåŠŸè¾¨è­˜ {detected_count} å¹€äººè‡‰ã€‚")
        
        if not session_history:
            return {"error": "No face detected (Server tried 3 rotations but failed). Try better lighting."}

        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_scores = {cls: 0.0 for cls in CLASSES}
        for entry in session_history:
            for cls in CLASSES:
                avg_scores[cls] += entry[cls]
                
        final_scores_float = {}
        for cls in CLASSES:
            final_scores_float[cls] = (avg_scores[cls] / len(session_history)) * 100
        
        final_scores_int = {k: int(v) for k, v in final_scores_float.items()}
        print(f"ğŸ“ˆ åˆ†æçµæœ: {final_scores_int}")

        # ç”Ÿæˆ AI è©•èª
        feedback_json = await generate_ai_feedback(final_scores_float)

        # è™•ç†å½±ç‰‡ URL
        video_url = None
        if save_video:
            filename = os.path.basename(video_path)
            video_url = f"http://10.0.2.2:8000/static/videos/{filename}"
        else:
            try:
                os.remove(video_path)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤æš«å­˜å½±ç‰‡: {video_path}")
            except Exception as del_err:
                print(f"âš ï¸ åˆªé™¤æš«å­˜å½±ç‰‡å¤±æ•—: {del_err}")

        return {
            "emotions": final_scores_int,
            "timeline": timeline_data,
            "ai_analysis": feedback_json,
            "video_url": video_url
        }

    except Exception as e:
        print(f"âŒ ä¼ºæœå™¨ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        return {"error": f"Server Error: {str(e)}"}


async def generate_ai_feedback(final_scores_float: dict) -> dict:
    """ç”Ÿæˆ AI è©•èª"""
    feedback_json = {
        "overall_score": 0,
        "comment": "åˆ†æå®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆè©•èª...",
        "suggestion": ""
    }
    
    # â˜…â˜…â˜… æš«æ™‚è·³é OpenAIï¼Œç›´æ¥ä½¿ç”¨æ•‘æ´è©•èªï¼ˆé¿å…ä¼ºæœå™¨ç•¶æ©Ÿï¼‰â˜…â˜…â˜…
    SKIP_OPENAI = True
    
    try:
        if SKIP_OPENAI:
            raise Exception("æš«æ™‚è·³é OpenAIï¼Œä½¿ç”¨æœ¬åœ°è©•èª")
            
        model_name = 'gpt-4o-mini'
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½é ‚å°–çš„å¤§å­¸å…¥å­¸é¢è©¦åŸ¹è¨“å°ˆå®¶ï¼ŒåŒæ™‚ä¹Ÿæ˜¯ä¸€ä½å°ˆæ¥­çš„è¡¨é”æºé€šæ•™ç·´ã€‚
        ä½ æ­£åœ¨ä¸€å°ä¸€æŒ‡å°ä¸€ä½é«˜ä¸­ç”Ÿï¼Œå¹«åŠ©ä»–åœ¨å‡å­¸é¢è©¦ä¸­è„«ç©è€Œå‡ºã€‚

        ã€AI è¡¨æƒ…åˆ†æçµæœã€‘ï¼ˆæœ¬æ¬¡æ¨¡æ“¬é¢è©¦çš„å¹³å‡æƒ…ç·’ä½”æ¯”ï¼‰
        - è‡ªä¿¡æŒ‡æ•¸: {final_scores_float.get('confidence', 0):.0f}%
        - ç†±å¿±æŒ‡æ•¸: {final_scores_float.get('passion', 0):.0f}%
        - æ”¾é¬†æŒ‡æ•¸: {final_scores_float.get('relaxed', 0):.0f}%
        - ç·Šå¼µæŒ‡æ•¸: {final_scores_float.get('nervous', 0):.0f}%

        ã€ä½ çš„ä»»å‹™ã€‘
        è«‹ç›´æ¥å°é€™ä½å­¸ç”Ÿèªªè©±ï¼ˆç”¨ã€Œä½ ã€ç¨±å‘¼ï¼‰ï¼Œçµ¦ä»–ä¸€ä»½**è¶…ç´šå¯¦ç”¨**çš„å›é¥‹ã€‚

        ğŸš« ç¦æ­¢äº‹é …ï¼ˆéå¸¸é‡è¦ï¼ï¼‰ï¼š
        - ä¸è¦åªæ˜¯é‡è¤‡èªªã€Œä½ çš„è‡ªä¿¡æŒ‡æ•¸æ˜¯ XX%ã€é€™ç¨®å»¢è©±
        - ä¸è¦èªªã€Œä½ å±•ç¾äº†æ²ˆç©©çš„ä¸€é¢ã€ã€Œæƒ…ç·’æ³¢å‹•ä¸å¤§ã€é€™ç¨®æ²’ç‡Ÿé¤Šçš„è©±
        - ä¸è¦æ³›æ³›åœ°èªªã€Œå¤šç·´ç¿’å°±æœƒé€²æ­¥ã€

        âœ… å¿…é ˆåšåˆ°ï¼š
        - çµ¦å‡º**å…·é«”åˆ°å¯ä»¥ä»Šå¤©å°±åŸ·è¡Œ**çš„å»ºè­°ï¼ˆä¾‹å¦‚ï¼šç·´ç¿’æ™‚å°è‘—é¡å­å¾®ç¬‘ã€å›ç­”å‰å…ˆæ·±å‘¼å¸ 3 ç§’ï¼‰
        - é‡å°**é¢è©¦æŠ€å·§**çµ¦å»ºè­°ï¼ˆçœ¼ç¥æ¥è§¸ã€æ‰‹å‹¢é‹ç”¨ã€èªé€Ÿæ§åˆ¶ã€é–‹å ´ç™½è¨­è¨ˆï¼‰
        - åƒä¸€å€‹çœŸæ­£é—œå¿ƒå­¸ç”Ÿçš„æ•™ç·´é‚£æ¨£èªªè©±ï¼Œæœ‰æº«åº¦ä½†ç›´æ¥

        ã€è¼¸å‡ºæ ¼å¼ã€‘
        è«‹åªå›å‚³ä¸€å€‹ JSONï¼Œä¸è¦æœ‰ä»»ä½• Markdown æ¨™è¨˜ï¼š
        {{
            "overall_score": (0-100 æ•´æ•¸ï¼Œæ ¹æ“šè‡ªä¿¡+ç†±å¿±çš„è¡¨ç¾çµ¦åˆ†ï¼Œç·Šå¼µé«˜è¦æ‰£åˆ†),
            "comment": (50-80 å­—çš„çŸ­è©•ï¼Œå‘Šè¨´å­¸ç”Ÿä»–é€™æ¬¡è¡¨ç¾çš„äº®é»å’Œéœ€è¦æ”¹é€²çš„åœ°æ–¹ï¼Œè¦å…·é«”ã€æœ‰æº«åº¦ï¼Œä¸è¦å»¢è©±),
            "suggestion": (ä¸€å¥è©±çš„å…·é«”è¡Œå‹•å»ºè­°ï¼Œä¾‹å¦‚ã€Œä¸‹æ¬¡å›ç­”å•é¡Œå‰ï¼Œå…ˆå°é¢è©¦å®˜å¾®ç¬‘ä¸¦é»é ­ï¼Œå†é–‹å§‹èªªè©±ã€)
        }}
        """

        if client:
            print(f"ğŸ¤– æ­£åœ¨å‘¼å« OpenAI ({model_name})...")
            
            # ä½¿ç”¨ asyncio.to_thread åœ¨èƒŒæ™¯åŸ·è¡ŒåŒæ­¥ OpenAI å‘¼å«
            import asyncio
            
            def call_openai():
                return client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    timeout=30,  # 30 ç§’è¶…æ™‚
                )
            
            try:
                response = await asyncio.to_thread(call_openai)
                content = response.choices[0].message.content
                clean_text = content.replace('```json', '').replace('```', '').strip()
                feedback_json = json.loads(clean_text)
                print("ğŸ“ AI è©•èªç”ŸæˆæˆåŠŸï¼")
            except Exception as openai_err:
                print(f"âš ï¸ OpenAI å‘¼å«å¤±æ•—: {openai_err}")
                raise openai_err
        else:
            raise Exception("OpenAI Client not initialized")

    except Exception as e:
        print(f"âš ï¸ AI ç”Ÿæˆå‡ºç¾ç‹€æ³ ({e})ï¼Œæ­£åœ¨å•Ÿå‹•è‡ªå‹•æ•‘æ´æ¨¡å¼...")
        
        c_score = int(final_scores_float.get('confidence', 0))
        n_score = int(final_scores_float.get('nervous', 0))
        p_score = int(final_scores_float.get('passion', 0))
        
        calc_score = 70 + (c_score * 0.3) + (p_score * 0.2) - (n_score * 0.2)
        calc_score = int(min(max(calc_score, 65), 96))

        if c_score >= 50:
            fallback_comment = "ä½ çš„è¡¨ç¾ç›¸ç•¶ç©©å¥ï¼Œçœ¼ç¥æ¥è§¸å……æ»¿è‡ªä¿¡ï¼Œçµ¦äººç•™ä¸‹äº†å¾ˆå¥½çš„ç¬¬ä¸€å°è±¡ã€‚æ•´é«”æ°›åœæ§åˆ¶å¾—å®œï¼Œå±•ç¾äº†ä¸éŒ¯çš„æŠ—å£“æ€§ï¼Œæ˜¯ä¸€ä½å¾ˆæœ‰æ½›åŠ›çš„è€ƒç”Ÿã€‚"
            fallback_suggestion = "å¯ä»¥å˜—è©¦åœ¨å›ç­”æ™‚åŠ å…¥æ›´å¤šå…·é«”çš„å€‹äººç¶“æ­·ï¼Œè®“å…§å®¹æ›´å…·èªªæœåŠ›ï¼Œä¸¦ä¿æŒç›®å‰çš„è‡ªä¿¡å§¿æ…‹ã€‚"
        elif n_score >= 30:
            fallback_comment = "é¢è©¦éç¨‹ä¸­ä½ çœ‹èµ·ä¾†æœ‰äº›è¨±ç·Šå¼µï¼Œå°è‡´è¡¨æƒ…ç•¥é¡¯åƒµç¡¬ï¼Œé€™åœ¨æ¨¡æ“¬é¢è©¦ä¸­æ˜¯å¾ˆæ­£å¸¸çš„ã€‚ä¸éä½ çš„æ…‹åº¦ä¾ç„¶èª æ‡‡ï¼Œåªè¦å¤šåŠ ç·´ç¿’ï¼Œå®šèƒ½å…‹æœç„¦æ…®ã€‚"
            fallback_suggestion = "å»ºè­°ç·´ç¿’æ·±å‘¼å¸æ”¾é¬†æ³•ï¼Œä¸¦è©¦è‘—åœ¨é¡å­å‰å¤šç·´ç¿’å¾®ç¬‘ï¼Œå¢åŠ è¦ªå’ŒåŠ›ï¼Œé¿å…å› ç·Šå¼µè€Œå¿˜è©ã€‚"
        elif p_score >= 30:
            fallback_comment = "ä½ è«‡è«–åˆ°ç›¸é—œè©±é¡Œæ™‚å±•ç¾äº†ä¸éŒ¯çš„ç†±å¿±ï¼Œé€™é»éå¸¸å¸å¼•äººã€‚ä¸éåœ¨å…¶ä»–éƒ¨åˆ†å¯ä»¥å†æ”¾é¬†ä¸€äº›ï¼Œè®“æ•´é«”è¡¨ç¾æ›´ç‚ºè‡ªç„¶æµæš¢ã€‚"
            fallback_suggestion = "è©¦è‘—å°‡é€™ä»½ç†±æƒ…å»¶ä¼¸åˆ°è‡ªæˆ‘ä»‹ç´¹ä¸­ï¼Œä¸¦æ³¨æ„èªé€Ÿçš„æ§åˆ¶ï¼Œè®“é¢è©¦å®˜èƒ½æ›´æ¸…æ¥šæ¥æ”¶ä½ çš„è¨Šæ¯ã€‚"
        else:
            fallback_comment = "æ•´å ´é¢è©¦è¡¨ç¾ä¸­è¦ä¸­çŸ©ï¼Œæƒ…ç·’æ³¢å‹•ä¸å¤§ï¼Œå±•ç¾äº†æ²ˆç©©çš„ä¸€é¢ã€‚é›–ç„¶æ²’æœ‰å¤ªå¤šå¤±èª¤ï¼Œä½†ä¹Ÿå°‘äº†äº›è¨±è¨˜æ†¶é»ï¼Œå»ºè­°å±•ç¾æ›´å¤šå°è©²é ˜åŸŸçš„ä¼åœ–å¿ƒã€‚"
            fallback_suggestion = "å›ç­”å•é¡Œæ™‚å¯ä»¥é©åº¦åŠ å¼·èªæ°£çš„æŠ‘æšé “æŒ«ï¼Œä¸¦å¤šé‹ç”¨æ‰‹å‹¢è¼”åŠ©ï¼Œè®“é¢è©¦å®˜æ„Ÿå—åˆ°ä½ çš„ç©æ¥µåº¦ã€‚"

        feedback_json = {
            "overall_score": calc_score,
            "comment": fallback_comment,
            "suggestion": fallback_suggestion
        }
        print(f"âœ… å·²å•Ÿç”¨æ•‘æ´è©•èª (åˆ†æ•¸: {calc_score})")
    
    return feedback_json


async def analyze_portfolio(pdf_path: str) -> dict:
    """
    åˆ†æå­¸ç¿’æ­·ç¨‹ PDF
    
    Args:
        pdf_path: PDF æª”æ¡ˆè·¯å¾‘
        
    Returns:
        åˆ†æçµæœ dict
    """
    try:
        # æå– PDF æ–‡å­—å…§å®¹
        try:
            import pdfplumber
            text_content = ""
            with pdfplumber.open(pdf_path) as pdf:
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text_content += page_text + "\n"
            
            print(f"ğŸ“– æå–åˆ° {len(text_content)} å­—")
            
            if len(text_content.strip()) < 50:
                os.remove(pdf_path)
                return {"error": "PDF å…§å®¹éå°‘æˆ–ç‚ºç´”åœ–ç‰‡æ ¼å¼ï¼Œç„¡æ³•åˆ†æã€‚è«‹ä¸Šå‚³åŒ…å«æ–‡å­—çš„ PDFã€‚"}
            
        except Exception as pdf_err:
            os.remove(pdf_path)
            print(f"âŒ PDF è§£æå¤±æ•—: {pdf_err}")
            return {"error": f"PDF è§£æå¤±æ•—: {str(pdf_err)}ã€‚è«‹ç¢ºèªå·²å®‰è£ pdfplumber"}
        
        if not client:
            os.remove(pdf_path)
            return {"error": "OpenAI API æœªè¨­å®š"}
        
        # é™åˆ¶æ–‡å­—é•·åº¦
        max_chars = 10000
        if len(text_content) > max_chars:
            text_content = text_content[:max_chars] + "\n...(å…§å®¹éé•·ï¼Œå·²æˆªæ–·)"
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é«˜ä¸­å‡å¤§å­¸è¼”å°å°ˆå®¶ï¼ŒåŒæ™‚ä¹Ÿæ˜¯æ•™è‚²éƒ¨ã€Œå­¸ç¿’æ­·ç¨‹æª”æ¡ˆã€çš„å¯©é–±å§”å“¡ã€‚
        ä½ æ­£åœ¨å¯©é–±ä¸€ä½é«˜ä¸­ç”Ÿçš„å­¸ç¿’æ­·ç¨‹æª”æ¡ˆï¼Œè«‹çµ¦äºˆå°ˆæ¥­çš„è©•åƒ¹å’Œå…·é«”çš„æ”¹é€²å»ºè­°ã€‚

        ã€å­¸ç¿’æ­·ç¨‹å…§å®¹ã€‘
        {text_content}

        ã€è«‹ä¾ç…§ä»¥ä¸‹æ ¼å¼çµ¦äºˆè©•åƒ¹ã€‘
        è«‹åªå›å‚³ä¸€å€‹ JSONï¼Œä¸è¦æœ‰ä»»ä½• Markdown æ¨™è¨˜ï¼š
        {{
            "overall_score": (0-100 æ•´æ•¸ï¼Œæ ¹æ“šå…§å®¹å®Œæ•´æ€§ã€å€‹äººç‰¹è‰²ã€åæ€æ·±åº¦çµ¦åˆ†),
            "strengths": [
                "å„ªé»1",
                "å„ªé»2",
                "å„ªé»3"
            ],
            "weaknesses": [
                "éœ€æ”¹é€²1",
                "éœ€æ”¹é€²2"
            ],
            "comment": (100-150å­—çš„æ•´é«”è©•èªï¼ŒæŒ‡å‡ºé€™ä»½å­¸ç¿’æ­·ç¨‹çš„äº®é»å’Œå¯ä»¥åŠ å¼·çš„åœ°æ–¹ï¼Œè¦å…·é«”ã€æœ‰å»ºè¨­æ€§),
            "suggestions": [
                "å…·é«”æ”¹é€²å»ºè­°1ï¼ˆä¾‹å¦‚ï¼šå¯ä»¥è£œå……å¯¦ä½œéç¨‹ä¸­é‡åˆ°çš„å›°é›£å’Œè§£æ±ºæ–¹æ³•ï¼‰",
                "å…·é«”æ”¹é€²å»ºè­°2",
                "å…·é«”æ”¹é€²å»ºè­°3"
            ]
        }}
        """
        
        print("ğŸ¤– æ­£åœ¨å‘¼å« OpenAI åˆ†æå­¸ç¿’æ­·ç¨‹...")
        
        response = client.chat.completions.create(
            model='gpt-4o-mini',
            messages=[
                {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
        )
        
        content = response.choices[0].message.content
        clean_text = content.replace('```json', '').replace('```', '').strip()
        result_json = json.loads(clean_text)
        print(f"âœ… å­¸ç¿’æ­·ç¨‹åˆ†æå®Œæˆï¼åˆ†æ•¸: {result_json.get('overall_score', 'N/A')}")
        
        # åˆªé™¤æš«å­˜ PDF
        os.remove(pdf_path)
        
        return {
            "success": True,
            "analysis": result_json
        }
        
    except Exception as e:
        print(f"âŒ å­¸ç¿’æ­·ç¨‹åˆ†æå¤±æ•—: {e}")
        traceback.print_exc()
        return {"error": f"åˆ†æå¤±æ•—: {str(e)}"}
