# emotion_service.py
# æƒ…ç·’åˆ†ææ ¸å¿ƒæœå‹™ - éåŒæ­¥ + å¤šç·šç¨‹ç‰ˆæœ¬

import os
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
import httpx
from dotenv import load_dotenv
import traceback
from collections import deque
import uuid
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# ---------------------------
# å…¨åŸŸè¨­å®š
# ---------------------------
PROJECT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
MODEL_PATH = os.path.join(PROJECT_DIR, "models", "test_best_.pth")
VIDEO_STORAGE_DIR = os.path.join(PROJECT_DIR, "static", "videos")
os.makedirs(VIDEO_STORAGE_DIR, exist_ok=True)

# OpenAI API Key
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

if OPENAI_API_KEY:
    print(f"ğŸ”‘ OpenAI API Key å‰åç¢¼: {OPENAI_API_KEY[:10]}...")
    print("âœ… OpenAI API è¨­å®šæˆåŠŸ")
else:
    print("âš ï¸ è­¦å‘Šï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYï¼ŒAI è©•èªåŠŸèƒ½å°‡ä½¿ç”¨æœ¬åœ°è©•èª")

# è¼‰å…¥äººè‡‰è¾¨è­˜å™¨
HAAR_PATH = os.path.join(PROJECT_DIR, "haarcascade_frontalface_default.xml")
if not os.path.exists(HAAR_PATH):
    print(f"âš ï¸ æœ¬åœ°æ‰¾ä¸åˆ° {HAAR_PATH}ï¼Œå˜—è©¦ä½¿ç”¨ OpenCV å…§å»ºè·¯å¾‘...")
    HAAR_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'

print(f"ğŸ“‚ æ­£åœ¨è¼‰å…¥äººè‡‰è¾¨è­˜æª”ï¼š{HAAR_PATH}")
face_cascade = cv2.CascadeClassifier(HAAR_PATH)

if face_cascade.empty():
    print("âŒ åš´é‡éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥äººè‡‰è¾¨è­˜å™¨ (xml æª”æ¡ˆææ¯€æˆ–è·¯å¾‘éŒ¯èª¤)")

# è¼‰å…¥æƒ…ç·’æ¨¡å‹ (ResNet18)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CLASSES = ['confidence', 'nervous', 'passion', 'relaxed']

model = models.resnet18(pretrained=False)
try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    state_dict = checkpoint["state_dict"] if "state_dict" in checkpoint else checkpoint
    
    fc_keys = [k for k in state_dict.keys() if k.startswith("fc.")]
    use_sequential = any(k.startswith("fc.1.") for k in fc_keys)
    
    if use_sequential:
        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, len(CLASSES)))
    else:
        model.fc = nn.Linear(model.fc.in_features, len(CLASSES))
        
    model.load_state_dict(state_dict, strict=False)
    print("âœ… æƒ…ç·’è¾¨è­˜æ¨¡å‹è¼‰å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    model.fc = nn.Linear(model.fc.in_features, len(CLASSES))

model = model.to(device)
model.eval()

# å½±åƒé è™•ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# â˜…â˜…â˜… å»ºç«‹å…±ç”¨çš„ ThreadPoolExecutor â˜…â˜…â˜…
# æœ€å¤šåŒæ™‚è™•ç† 4 å€‹å½±ç‰‡ä»»å‹™
executor = ThreadPoolExecutor(max_workers=4)


def get_video_storage_dir():
    """å–å¾—å½±ç‰‡å„²å­˜ç›®éŒ„"""
    return VIDEO_STORAGE_DIR


def _analyze_video_sync(video_path: str, save_video: bool) -> dict:
    """åŒæ­¥è™•ç†å½±ç‰‡çš„æ ¸å¿ƒé‚è¼¯ (åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œ)"""
    try:
        print(f"ğŸ¬ [Worker] é–‹å§‹è™•ç†å½±ç‰‡: {video_path}")
        cap = cv2.VideoCapture(video_path)
        
        if not cap.isOpened():
            return {"error": "Could not open video"}

        timeline_data = []
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0 or fps is None:
            fps = 30
        frame_interval = max(1, int(fps / 3))

        session_history = []
        frame_count = 0
        detected_count = 0
        
        orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"ğŸ¥ åŸå§‹å½±ç‰‡å°ºå¯¸: {orig_w} x {orig_h}, FPS: {fps}")

        smooth_queue = deque(maxlen=5)

        with torch.no_grad():
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                if frame_count % 3 != 0:
                    continue

                # ç¸®å°åœ–ç‰‡ä»¥åŠ å¿«åµæ¸¬é€Ÿåº¦
                h_orig, w_orig = frame.shape[:2]
                if w_orig > 640:
                    scale = 640 / w_orig
                    frame_small = cv2.resize(frame, (640, int(h_orig * scale)))
                else:
                    frame_small = frame
                    
                gray = cv2.cvtColor(frame_small, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.1, 8)
                
                found_face_info = None
                if len(faces) > 0:
                     if w_orig > 640:
                        scale_inv = w_orig / 640
                        faces = [(int(x*scale_inv), int(y*scale_inv), int(w*scale_inv), int(h*scale_inv)) for (x,y,w,h) in faces]
                        found_face_info = (frame, faces)
                     else:
                        found_face_info = (frame, faces)

                if found_face_info is None:
                    continue

                detected_count += 1
                correct_frame, faces = found_face_info
                (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])

                face_crop = correct_frame[y:y+h, x:x+w]

                try:
                    img = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
                    img = Image.fromarray(img)
                    img_tensor = transform(img).unsqueeze(0).to(device)

                    outputs = model(img_tensor)
                    probs = torch.softmax(outputs, dim=1)[0]
                    
                    smooth_queue.append(probs.cpu())
                    avg_probs = torch.stack(list(smooth_queue), dim=0).mean(dim=0)

                    current_emotions = {}
                    for i, cls in enumerate(CLASSES):
                        current_emotions[cls] = avg_probs[i].item()
                    
                    session_history.append(current_emotions)

                    if frame_count % frame_interval == 0:
                        timeline_entry = {
                            "t": round(frame_count / fps, 1),
                            "c": int(current_emotions['confidence'] * 100),
                            "n": int(current_emotions['nervous'] * 100),
                            "p": int(current_emotions['passion'] * 100),
                            "r": int(current_emotions['relaxed'] * 100)
                        }
                        timeline_data.append(timeline_entry)

                except Exception:
                    pass

        cap.release()
        print(f"ğŸ“Š [Worker] åˆ†æå®Œæˆï¼šå…± {frame_count} å¹€ï¼Œè¾¨è­˜ {detected_count} å¹€")
        
        if not session_history:
            return {"error": "No face detected. Please fetch camera directly to your face."}

        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_scores = {cls: 0.0 for cls in CLASSES}
        for entry in session_history:
            for cls in CLASSES:
                avg_scores[cls] += entry[cls]
                
        final_scores_float = {}
        for cls in CLASSES:
            final_scores_float[cls] = (avg_scores[cls] / len(session_history)) * 100
        
        final_scores_int = {k: int(v) for k, v in final_scores_float.items()}
        print(f"ğŸ“ˆ çµæœ: {final_scores_int}")

        # è™•ç†å½±ç‰‡ URL (å…ˆå›å‚³ï¼ŒAI è©•èªç¨å¾Œè™•ç†)
        video_url = None
        if save_video:
            filename = os.path.basename(video_path)
            video_url = f"http://10.0.2.2:8000/static/videos/{filename}"
        else:
            try:
                os.remove(video_path)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤æš«å­˜å½±ç‰‡")
            except:
                pass

        return {
            "emotions": final_scores_int,
            "timeline": timeline_data,
            "final_scores_float": final_scores_float,
            "video_url": video_url
        }

    except Exception as e:
        print(f"âŒ [Worker] åˆ†æéŒ¯èª¤: {e}")
        traceback.print_exc()
        return {"error": f"Error: {str(e)}"}


def _generate_ai_feedback_sync(final_scores_float: dict) -> dict:
    """åŒæ­¥ç”Ÿæˆ AI è©•èª (åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œ)"""
    try:
        if not OPENAI_API_KEY:
            raise Exception("ç„¡ API Key")
        
        confidence = final_scores_float.get('confidence', 0)
        passion = final_scores_float.get('passion', 0)
        relaxed = final_scores_float.get('relaxed', 0)
        nervous = final_scores_float.get('nervous', 0)
        
        # â˜…â˜…â˜… æ”¹é€²ç‰ˆæç¤ºè© â˜…â˜…â˜…
        prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„é¢è©¦åŸ¹è¨“æ•™ç·´ï¼Œæ­£åœ¨ç›´æ¥å°å­¸ç”Ÿèªªè©±ã€‚è«‹æ ¹æ“šä»¥ä¸‹é¢è©¦å¾®è¡¨æƒ…åˆ†æçµæœï¼Œæä¾›è©³ç´°ä¸”æœ‰å»ºè¨­æ€§çš„è©•ä¼°ã€‚

ã€é‡è¦ã€‘è«‹ä½¿ç”¨ã€Œä½ ã€ç›´æ¥å°å­¸ç”Ÿèªªè©±ï¼Œä¸è¦ç”¨ç¬¬ä¸‰äººç¨±ã€‚ä¾‹å¦‚ï¼šã€Œä½ çš„è¡¨ç¾å¾ˆå¥½ã€è€Œéã€Œå­¸ç”Ÿè¡¨ç¾å¾ˆå¥½ã€ã€‚

ã€æƒ…ç·’æ•¸æ“šåˆ†æã€‘
- è‡ªä¿¡ç¨‹åº¦: {confidence:.0f}%
- è¡¨é”ç†±å¿±: {passion:.0f}%
- æ”¾é¬†ç¨‹åº¦: {relaxed:.0f}%
- ç·Šå¼µç¨‹åº¦: {nervous:.0f}%

ã€è©•åˆ†æ¨™æº–ã€‘ï¼ˆè«‹ä¾æ­¤è¨ˆç®— overall_scoreï¼‰
1. åŸºç¤åˆ† 60 åˆ†
2. è‡ªä¿¡ â‰¥30% åŠ  15 åˆ†ï¼Œâ‰¥50% å†åŠ  10 åˆ†
3. ç†±å¿± â‰¥30% åŠ  10 åˆ†
4. æ”¾é¬† â‰¥30% åŠ  5 åˆ†
5. ç·Šå¼µ â‰¥20% æ‰£ 10 åˆ†ï¼Œâ‰¥35% æ‰£ 15 åˆ†
6. æœ€çµ‚åˆ†æ•¸é™åˆ¶åœ¨ 40-98 åˆ†ä¹‹é–“

ã€å›è¦†æ ¼å¼ã€‘
è«‹åªå›å‚³ç´” JSONï¼ˆä¸è¦ Markdown å€å¡Šï¼‰ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "overall_score": è¨ˆç®—å¾Œçš„æ•´æ•¸åˆ†æ•¸,
  "comment": "100-150å­—çš„ç¶œåˆè©•èªï¼Œç”¨ã€Œä½ ã€ç›´æ¥å°å­¸ç”Ÿèªªè©±ï¼Œéœ€åŒ…å«ï¼š(1) ä½ çš„è¡¨ç¾å„ªé» (2) ä½ éœ€è¦æ”¹é€²ä¹‹è™• (3) æ•´é«”è©•åƒ¹",
  "suggestion": "2-3 æ¢å…·é«”å¯åŸ·è¡Œçš„æ”¹é€²å»ºè­°ï¼Œç”¨ã€Œä½ ã€å°å­¸ç”Ÿèªªè©±ï¼Œç”¨åˆ†è™Ÿåˆ†éš”"
}}"""
            
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "gpt-3.5-turbo",
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯å°ˆæ¥­é¢è©¦æ•™ç·´ï¼Œæ“…é•·åˆ†æå¾®è¡¨æƒ…ä¸¦çµ¦äºˆå…·é«”å»ºè­°ã€‚è«‹åªå›å‚³ JSONï¼Œä¸è¦ä½¿ç”¨ Markdownã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
            "max_tokens": 512  # â˜… å¢åŠ  token ä¸Šé™
        }
        
        print("ğŸ¤– å‘¼å« OpenAI ç”Ÿæˆè©•èª (åŒæ­¥ï¼Œåœ¨ç¨ç«‹ç·šç¨‹ä¸­)...")
        
        # â˜…â˜…â˜… ä½¿ç”¨åŒæ­¥ httpx â˜…â˜…â˜…
        with httpx.Client(timeout=30.0) as client:
            resp = client.post(url, headers=headers, json=payload)
        
        if resp.status_code == 200:
            content = resp.json()["choices"][0]["message"]["content"]
            clean = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean)
        else:
            raise Exception(f"API Error {resp.status_code}")

    except Exception as e:
        print(f"âš ï¸ å•Ÿç”¨æ•‘æ´è©•èª: {e}")
        # â˜…â˜…â˜… æ”¹é€²ç‰ˆæ•‘æ´é‚è¼¯ï¼šä½¿ç”¨è©•åˆ†æ¨™æº–è¨ˆç®— â˜…â˜…â˜…
        c = int(confidence) if 'confidence' in dir() else int(final_scores_float.get('confidence', 0))
        p = int(final_scores_float.get('passion', 0))
        r = int(final_scores_float.get('relaxed', 0))
        n = int(final_scores_float.get('nervous', 0))
        
        calc_score = 60
        if c >= 30: calc_score += 15
        if c >= 50: calc_score += 10
        if p >= 30: calc_score += 10
        if r >= 30: calc_score += 5
        if n >= 20: calc_score -= 10
        if n >= 35: calc_score -= 15
        calc_score = int(min(max(calc_score, 40), 98))
        
        return {
            "overall_score": calc_score,
            "comment": f"ä½ çš„è‡ªä¿¡ç¨‹åº¦ç‚º {c}%ï¼Œæ•´é«”è¡¨ç¾{'è‰¯å¥½' if c >= 50 else 'å°šå¯'}ã€‚{'ç†±å¿±åº¦è¶³å¤ ï¼Œèƒ½æ„Ÿå—åˆ°ä½ å°é€™æ¬¡é¢è©¦çš„é‡è¦–ã€‚' if p >= 40 else 'å»ºè­°å±•ç¾æ›´å¤šç†±å¿±ã€‚'}{'ä½†ç·Šå¼µç¨‹åº¦è¼ƒé«˜ï¼Œå¯èƒ½å½±éŸ¿ç™¼æ®ã€‚' if n >= 50 else 'æƒ…ç·’æ§åˆ¶ç©©å®šã€‚'}å»ºè­°å¤šç·´ç¿’æ¨¡æ“¬é¢è©¦ä»¥æå‡è¡¨ç¾ã€‚",
            "suggestion": "é¢è©¦å‰åš 3 æ¬¡æ·±å‘¼å¸æ”¾é¬†ï¼›ç·´ç¿’å°é¡å­å›ç­”å•é¡Œï¼›æº–å‚™ 2-3 å€‹è‡ªå·±çš„æ•…äº‹æ¡ˆä¾‹"
        }


async def analyze_video(video_path: str, save_video: bool = True) -> dict:
    """
    éåŒæ­¥åˆ†æå½±ç‰‡
    - å½±ç‰‡è™•ç†ï¼šåœ¨ ThreadPoolExecutor ä¸­åŸ·è¡Œï¼ˆä¸é˜»å¡ä¸»ç·šç¨‹ï¼‰
    - AI è©•èªï¼šä¹Ÿåœ¨ ThreadPoolExecutor ä¸­åŸ·è¡Œ
    """
    loop = asyncio.get_event_loop()
    
    # â˜…â˜…â˜… ä½¿ç”¨ ThreadPoolExecutor åŸ·è¡Œå½±ç‰‡åˆ†æ â˜…â˜…â˜…
    # é€™æ¨£å³ä½¿å½±ç‰‡è™•ç†å´©æ½°ï¼Œä¹Ÿä¸æœƒå½±éŸ¿ä¸»ç¨‹å¼
    video_result = await loop.run_in_executor(executor, _analyze_video_sync, video_path, save_video)
    
    if "error" in video_result:
        return video_result
    
    # æå–åˆ†æçµæœ
    final_scores_float = video_result.pop("final_scores_float", {})
    
    # â˜…â˜…â˜… åœ¨ç¨ç«‹ç·šç¨‹ä¸­å‘¼å« OpenAI â˜…â˜…â˜…
    ai_feedback = await loop.run_in_executor(executor, _generate_ai_feedback_sync, final_scores_float)
    
    video_result["ai_analysis"] = ai_feedback
    return video_result


def _analyze_portfolio_sync(pdf_path: str) -> dict:
    """åŒæ­¥åˆ†æå­¸ç¿’æ­·ç¨‹ PDF (åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œ)"""
    try:
        # æå– PDF æ–‡å­—å…§å®¹
        try:
            from PyPDF2 import PdfReader
            text_content = ""
            
            reader = PdfReader(pdf_path)
            for page in reader.pages:
                t = page.extract_text()
                if t:
                    text_content += t + "\n"
            
            print(f"ğŸ“– æå–åˆ° {len(text_content)} å­—")
            
            if len(text_content.strip()) < 50:
                try: os.remove(pdf_path)
                except: pass
                return {"error": "PDF å…§å®¹éå°‘æˆ–ç‚ºç´”åœ–ç‰‡æ ¼å¼ï¼Œç„¡æ³•åˆ†æã€‚è«‹ä¸Šå‚³åŒ…å«æ–‡å­—çš„ PDFã€‚"}
            
        except Exception as pdf_err:
            try: os.remove(pdf_path)
            except: pass
            print(f"âŒ PDF è§£æå¤±æ•—: {pdf_err}")
            return {"error": f"PDF è§£æå¤±æ•—: {str(pdf_err)}"}
        
        if not OPENAI_API_KEY:
            try: os.remove(pdf_path)
            except: pass
            return {"error": "OpenAI API æœªè¨­å®š"}
        
        # é™åˆ¶æ–‡å­—é•·åº¦
        max_chars = 10000
        if len(text_content) > max_chars:
            text_content = text_content[:max_chars] + "\n...(å…§å®¹éé•·ï¼Œå·²æˆªæ–·)"
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„é«˜ä¸­å‡å¤§å­¸è¼”å°å°ˆå®¶ï¼ŒåŒæ™‚ä¹Ÿæ˜¯æ•™è‚²éƒ¨ã€Œå­¸ç¿’æ­·ç¨‹æª”æ¡ˆã€çš„å¯©é–±å§”å“¡ã€‚
        ä½ æ­£åœ¨å¯©é–±ä¸€ä½é«˜ä¸­ç”Ÿçš„å­¸ç¿’æ­·ç¨‹æª”æ¡ˆï¼Œè«‹çµ¦äºˆå°ˆæ¥­çš„è©•åƒ¹å’Œå…·é«”çš„æ”¹é€²å»ºè­°ã€‚

        ã€å­¸ç¿’æ­·ç¨‹å…§å®¹ã€‘
        {text_content}

        ã€è«‹ä¾ç…§ä»¥ä¸‹æ ¼å¼çµ¦äºˆè©•åƒ¹ã€‘
        è«‹åªå›å‚³ä¸€å€‹ JSONï¼Œä¸è¦æœ‰ä»»ä½• Markdown æ¨™è¨˜ï¼š
        {{
            "overall_score": (0-100 æ•´æ•¸ï¼Œæ ¹æ“šå…§å®¹å®Œæ•´æ€§ã€å€‹äººç‰¹è‰²ã€åæ€æ·±åº¦çµ¦åˆ†),
            "strengths": [
                "å„ªé»1",
                "å„ªé»2",
                "å„ªé»3"
            ],
            "weaknesses": [
                "éœ€æ”¹é€²1",
                "éœ€æ”¹é€²2"
            ],
            "comment": (100-150å­—çš„æ•´é«”è©•èªï¼ŒæŒ‡å‡ºé€™ä»½å­¸ç¿’æ­·ç¨‹çš„äº®é»å’Œå¯ä»¥åŠ å¼·çš„åœ°æ–¹ï¼Œè¦å…·é«”ã€æœ‰å»ºè¨­æ€§),
            "suggestions": [
                "å…·é«”æ”¹é€²å»ºè­°1",
                "å…·é«”æ”¹é€²å»ºè­°2",
                "å…·é«”æ”¹é€²å»ºè­°3"
            ]
        }}
        """
        
        print("ğŸ¤– æ­£åœ¨å‘¼å« OpenAI åˆ†æå­¸ç¿’æ­·ç¨‹ (åŒæ­¥ï¼Œåœ¨ç¨ç«‹ç·šç¨‹ä¸­)...")
        
        url = "https://api.openai.com/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {OPENAI_API_KEY}",
            "Content-Type": "application/json"
        }
        payload = {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.7,
        }
        
        # â˜…â˜…â˜… ä½¿ç”¨åŒæ­¥ httpx â˜…â˜…â˜…
        with httpx.Client(timeout=60.0) as client:
            resp = client.post(url, headers=headers, json=payload)
        
        if resp.status_code == 200:
            content = resp.json()["choices"][0]["message"]["content"]
            clean_text = content.replace('```json', '').replace('```', '').strip()
            result_json = json.loads(clean_text)
            print(f"âœ… å­¸ç¿’æ­·ç¨‹åˆ†æå®Œæˆï¼åˆ†æ•¸: {result_json.get('overall_score', 'N/A')}")
            
            try: os.remove(pdf_path)
            except: pass
            
            return {
                "success": True,
                "analysis": result_json
            }
        else:
             raise Exception(f"API Error: {resp.status_code}")
        
    except Exception as e:
        print(f"âŒ å­¸ç¿’æ­·ç¨‹åˆ†æå¤±æ•—: {e}")
        traceback.print_exc()
        return {"error": f"åˆ†æå¤±æ•—: {str(e)}"}


async def analyze_portfolio(pdf_path: str) -> dict:
    """éåŒæ­¥å…¥å£ - åœ¨ç¨ç«‹ç·šç¨‹ä¸­åŸ·è¡Œåˆ†æ"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, _analyze_portfolio_sync, pdf_path)
