# fileName: backend/app.py
import os
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
from flask import Flask, request, jsonify
from openai import OpenAI
from dotenv import load_dotenv
import sys
import traceback 
from collections import deque 

# 1. è¼‰å…¥ç’°å¢ƒè®Šæ•¸ (è®€å– .env)
load_dotenv()

app = Flask(__name__)

# -----------------------------
# 2. è¨­å®šèˆ‡åˆå§‹åŒ–
# -----------------------------
PROJECT_DIR = os.getcwd()
MODEL_PATH = os.path.join(PROJECT_DIR, "models", "test_best_.pth")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# æ–°çš„ (OpenAI)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = None

if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)
    print("âœ… OpenAI Client è¨­å®šæˆåŠŸ")
else:
    print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYï¼Œè«‹æª¢æŸ¥ .env æª”æ¡ˆ")

# è¼‰å…¥äººè‡‰è¾¨è­˜å™¨ (å„ªå…ˆè®€å–æœ¬åœ°ï¼Œè®€ä¸åˆ°æ‰è®€ç³»çµ±)
HAAR_PATH = os.path.join(PROJECT_DIR, "haarcascade_frontalface_default.xml")
if not os.path.exists(HAAR_PATH):
    print(f"âš ï¸ æœ¬åœ°æ‰¾ä¸åˆ° {HAAR_PATH}ï¼Œå˜—è©¦ä½¿ç”¨ OpenCV å…§å»ºè·¯å¾‘...")
    HAAR_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'

print(f"ğŸ“‚ æ­£åœ¨è¼‰å…¥äººè‡‰è¾¨è­˜æª”ï¼š{HAAR_PATH}")
face_cascade = cv2.CascadeClassifier(HAAR_PATH)

if face_cascade.empty():
    print("âŒ åš´é‡éŒ¯èª¤ï¼šç„¡æ³•è¼‰å…¥äººè‡‰è¾¨è­˜å™¨ (xml æª”æ¡ˆææ¯€æˆ–è·¯å¾‘éŒ¯èª¤)")

# è¼‰å…¥æƒ…ç·’æ¨¡å‹ (ResNet18)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
CLASSES = ['confidence', 'nervous', 'passion', 'relaxed'] 

model = models.resnet18(pretrained=False)
try:
    checkpoint = torch.load(MODEL_PATH, map_location=device)
    # è™•ç†ä¸åŒçš„ Checkpoint çµæ§‹
    state_dict = checkpoint["state_dict"] if "state_dict" in checkpoint else checkpoint
    
    # æª¢æŸ¥ fc å±¤çµæ§‹
    fc_keys = [k for k in state_dict.keys() if k.startswith("fc.")]
    use_sequential = any(k.startswith("fc.1.") for k in fc_keys)
    
    if use_sequential:
        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, len(CLASSES)))
    else:
        model.fc = nn.Linear(model.fc.in_features, len(CLASSES))
        
    model.load_state_dict(state_dict, strict=False)
    print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
except Exception as e:
    print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
    # å»ºç«‹ç©ºæ¨¡å‹ä»¥é˜²å´©æ½°
    model.fc = nn.Linear(model.fc.in_features, len(CLASSES))

model = model.to(device)
model.eval()

# å½±åƒé è™•ç† (èˆ‡ä½ çš„ PC ç‰ˆä¿æŒå®Œå…¨ä¸€è‡´)
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# -----------------------------
# 3. è™•ç†å½±ç‰‡ API (æš´åŠ›å…¨è§’åº¦æœå°‹ç‰ˆ)
# -----------------------------
@app.route('/analyze', methods=['POST'])
def analyze_video():
    try:
        # 1. ç¬¬ä¸€æ­¥ï¼šå…ˆæª¢æŸ¥ä¸¦å„²å­˜å½±ç‰‡ (ä¸€å®šè¦æœ€å…ˆåšï¼)
        if 'video' not in request.files:
            return jsonify({"error": "No video file provided"}), 400
        
        video_file = request.files['video']
        save_path = os.path.join(PROJECT_DIR, "temp_upload.mp4")
        video_file.save(save_path)
        print("ğŸ“¥ æ”¶åˆ°å½±ç‰‡ï¼Œé–‹å§‹åˆ†æ...")

        # 2. ç¬¬äºŒæ­¥ï¼šå½±ç‰‡å­˜å¥½äº†ï¼Œæ‰èƒ½å®£å‘Š cap (æ‰“é–‹å½±ç‰‡)
        cap = cv2.VideoCapture(save_path)
        
        if not cap.isOpened():
             return jsonify({"error": "Could not open video"}), 500

        # 3. ç¬¬ä¸‰æ­¥ï¼šæœ‰äº† capï¼Œæ‰èƒ½è®€å– FPS å’Œåˆå§‹åŒ–è®Šæ•¸ (é€™äº›åŸæœ¬è¢«ä½ æ”¾åœ¨æœ€ä¸Šé¢)
        timeline_data = [] 
        fps = cap.get(cv2.CAP_PROP_FPS)
        if fps == 0 or fps is None: fps = 30
        frame_interval = int(fps) # ç”¨ä¾†åšæ™‚é–“è»¸è¨˜éŒ„

        # åˆå§‹åŒ–å…¶ä»–è®Šæ•¸
        session_history = []
        frame_count = 0
        detected_count = 0
        
        # å–å¾—å½±ç‰‡åŸå§‹å°ºå¯¸
        orig_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        orig_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        print(f"ğŸ¥ åŸå§‹å½±ç‰‡å°ºå¯¸: {orig_w} x {orig_h}")

        # â˜…â˜…â˜… å¹³æ»‘éšŠåˆ— â˜…â˜…â˜…
        smooth_queue = deque(maxlen=5) # é€™è£¡ç›´æ¥å®£å‘Šå°±å¥½ï¼Œä¸ç”¨ check locals

        with torch.no_grad():
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                if frame_count % 3 != 0: continue # è·³å¹€è™•ç†ï¼ŒåŠ å¿«é€Ÿåº¦ (æ¯3å¹€å–1å¹€)

                # -------------------------------------------------------------
                # ã€æš´åŠ›ä¿®æ­£å€å¡Šã€‘å¤šè§’åº¦äººè‡‰æœå°‹
                # æˆ‘å€‘ä¸å†çŒœæ¸¬å½±ç‰‡æ˜¯å¦éœ€è¦æ—‹è½‰ï¼Œè€Œæ˜¯ç›´æ¥å˜—è©¦ä¸‰ç¨®è§’åº¦ï¼š
                # 1. åŸå§‹  2. é †æ™‚é‡90åº¦  3. é€†æ™‚é‡90åº¦
                # -------------------------------------------------------------
                
                found_face_info = None # ç”¨ä¾†å­˜ (æ­£ç¢ºè§’åº¦çš„frame, faces)

                # å®šç¾©è¦å˜—è©¦çš„æ“ä½œæ¸…å–® (Noneä»£è¡¨ä¸è½‰)
                rotation_attempts = [None, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE]
                
                for rot_code in rotation_attempts:
                    # è¤‡è£½ä¸€ä»½ç›®å‰çš„ frame ä¾†è½‰ï¼Œé¿å…æ±™æŸ“åŸåœ–
                    temp_frame = frame.copy()
                    
                    if rot_code is not None:
                        temp_frame = cv2.rotate(temp_frame, rot_code)
                    
                    # çµ±ä¸€ç¸®æ”¾ (é¿å…åœ–ç‰‡å¤ªå¤§ Haar è·‘ä¸å‹•ï¼Œä¹Ÿé¿å…å¤ªå°æŠ“ä¸åˆ°)
                    # é€™è£¡å¼·åˆ¶é–å®šå¯¬åº¦ 480 (æ¯”ä¹‹å‰çš„ 360 å¤§ä¸€é»ï¼Œå¢åŠ è¾¨è­˜ç‡)
                    target_w = 480
                    h_curr, w_curr, _ = temp_frame.shape
                    scale = target_w / w_curr
                    new_h = int(h_curr * scale)
                    temp_frame = cv2.resize(temp_frame, (target_w, new_h))
                    
                    gray = cv2.cvtColor(temp_frame, cv2.COLOR_BGR2GRAY)
                    
                    # â˜…â˜…â˜… ä½¿ç”¨èˆ‡ä½  PC ç‰ˆå®Œå…¨ç›¸åŒçš„åƒæ•¸ (1.1, 8) â˜…â˜…â˜…
                    # é€™èƒ½ç¢ºä¿åªè¦ PC ç‰ˆèƒ½æŠ“åˆ°ï¼ŒServer ç‰ˆå°±èƒ½æŠ“åˆ°
                    faces = face_cascade.detectMultiScale(gray, 1.1, 8)
                    
                    if len(faces) > 0:
                        # æ‰¾åˆ°äº†ï¼è¨˜éŒ„ä¸‹ä¾†ä¸¦è·³å‡ºè¿´åœˆ
                        found_face_info = (temp_frame, faces)
                        break 
                
                # å¦‚æœè½‰äº†ä¸‰åœˆé‚„æ˜¯æ²’è‡‰ï¼Œå°±æ”¾æ£„é€™ä¸€å¹€
                if found_face_info is None:
                    continue

                detected_count += 1
                
                # å–å‡ºæ­£ç¢ºè§’åº¦çš„åœ–å’Œäººè‡‰åº§æ¨™
                correct_frame, faces = found_face_info
                
                # æ‰¾æœ€å¤§çš„äººè‡‰
                (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])

                # å­˜æª”å‰å¹¾å¼µ Debug ç”¨ (ç¢ºèªé€™æ¬¡è½‰å°äº†å—)
                if detected_count <= 3: 
                    debug_frame = correct_frame.copy()
                    cv2.rectangle(debug_frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.imwrite(f"debug_face_server_{detected_count}.jpg", debug_frame)

                # è£åˆ‡ (ä¸åšé¡å¤– Paddingï¼Œä¿æŒèˆ‡ PC ç‰ˆé‚è¼¯ä¸€è‡´)
                face_crop = correct_frame[y:y+h, x:x+w]

                try:
                    # è½‰ RGB -> PIL -> Tensor
                    img = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
                    img = Image.fromarray(img)
                    img_tensor = transform(img).unsqueeze(0).to(device)

                    outputs = model(img_tensor)
                    probs = torch.softmax(outputs, dim=1)[0]
                    
                    # å¹³æ»‘é‹ç®—
                    smooth_queue.append(probs.cpu())
                    avg_probs = torch.stack(list(smooth_queue), dim=0).mean(dim=0)

                    current_emotions = {}
                    for i, cls in enumerate(CLASSES):
                        current_emotions[cls] = avg_probs[i].item()
                    
                    session_history.append(current_emotions)

                    if frame_count % frame_interval == 0:
                        timeline_entry = {
                            "t": round(frame_count / fps, 1), # æ™‚é–“ (ç§’)
                            "c": int(current_emotions['confidence'] * 100),
                            "n": int(current_emotions['nervous'] * 100),
                            "p": int(current_emotions['passion'] * 100),
                            "r": int(current_emotions['relaxed'] * 100)
                        }
                        timeline_data.append(timeline_entry)

                except Exception as img_err:
                    print(f"âš ï¸ å½±åƒè™•ç†éŒ¯èª¤: {img_err}")
                    pass

        cap.release()
        print(f"ğŸ“Š åˆ†æå®Œæˆï¼šå…±è®€å– {frame_count} å¹€ï¼ŒæˆåŠŸè¾¨è­˜ {detected_count} å¹€äººè‡‰ã€‚")
        
        # å¦‚æœå®Œå…¨æ²’æŠ“åˆ°è‡‰
        if not session_history:
            return jsonify({
                "error": "No face detected (Server tried 3 rotations but failed). Try better lighting."
            }), 400

        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_scores = {cls: 0.0 for cls in CLASSES}
        for entry in session_history:
            for cls in CLASSES:
                avg_scores[cls] += entry[cls]
                
        final_scores_float = {}
        for cls in CLASSES:
            final_scores_float[cls] = (avg_scores[cls] / len(session_history)) * 100
        
        # è½‰æˆæ•´æ•¸
        final_scores_int = {k: int(v) for k, v in final_scores_float.items()}
        print(f"ğŸ“ˆ åˆ†æçµæœ: {final_scores_int}")

       # -----------------------------
        # 4. Gemini è©•èª (é›™é‡ä¿éšªç‰ˆï¼šAI å¤±æ•—æ™‚è‡ªå‹•åˆ‡æ›å‚™ç”¨è©•èª)
        # -----------------------------
        feedback_json = {
            "overall_score": 0, 
            "comment": "åˆ†æå®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆè©•èª...", 
            "suggestion": ""
        }
        
        try:
            # â˜…â˜…â˜… è¨­å®šæ¨¡å‹ï¼šä½¿ç”¨ç©©å®šç‰ˆ 1.5-flash â˜…â˜…â˜…
            model_name = 'gpt-4o'
            
            # é€™æ˜¯ä½ è¦æ±‚çš„å®Œæ•´ Promptï¼Œå®Œå…¨ä¿ç•™
            prompt = f"""
            ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¤§å­¸å…¥å­¸é¢è©¦æ•™ç·´ã€‚ä½ å‰›å‰›è§€å¯Ÿäº†ä¸€ä½é«˜ä¸­ç”Ÿçš„æ¨¡æ“¬é¢è©¦è¡¨ç¾ã€‚
            ä»¥ä¸‹æ˜¯é€é AI å¾®è¡¨æƒ…åˆ†æç³»çµ±åµæ¸¬åˆ°çš„æƒ…ç·’æ•¸æ“šï¼ˆæ•´å ´é¢è©¦çš„å¹³å‡ä½”æ¯”ï¼‰ï¼š

            ã€æƒ…ç·’æ•¸æ“šã€‘
            - Confidence (è‡ªä¿¡): {final_scores_float.get('confidence', 0):.1f}%
            - Passion (ç†±å¿±): {final_scores_float.get('passion', 0):.1f}%
            - Relaxed (æ²ˆç©©/åŸºæº–ç·š): {final_scores_float.get('relaxed', 0):.1f}%
            - Nervous (ç·Šå¼µ/ç„¦æ…®): {final_scores_float.get('nervous', 0):.1f}%

            ã€æƒ…ç·’å®šç¾©åƒè€ƒã€‘
            1. Confidence: çœ¼ç¥å …å®šã€æœ‰è‡ªä¿¡ã€‚
            2. Passion: è«‡è«–èˆˆè¶£æ™‚å±•ç¾çš„ç†±æƒ…ã€‚
            3. Relaxed: å°ˆæ³¨è†è½æˆ–æƒ…ç·’å¹³ç©©ï¼ˆåŸºæº–ç·šï¼‰ã€‚
            4. Nervous: ç„¦æ…®ã€åƒµç¡¬æˆ–ä¸è‡ªç„¶ã€‚

            ã€ä»»å‹™ã€‘
            è«‹æ ¹æ“šä»¥ä¸Šæ•¸æ“šï¼Œç›´æ¥å°è‘—é€™ä½è€ƒç”Ÿï¼ˆä½¿ç”¨ã€Œä½ ã€ä¾†ç¨±å‘¼ï¼‰ï¼Œç”Ÿæˆä¸€ä»½ç°¡çŸ­æœ‰åŠ›çš„ã€Œé¢è©¦è¡¨ç¾åˆ†æå ±å‘Šã€ã€‚
            è«‹åŒ…å«ä»¥ä¸‹ä¸‰å€‹éƒ¨åˆ†ï¼š
            1. **æ•´é«”è¡¨ç¾è©•åˆ†**ï¼šæ ¹æ“šè‡ªä¿¡èˆ‡ç†±å¿±çš„æ¯”ä¾‹ï¼Œçµ¦ã€Œä½ ã€ä¸€å¥ç¸½è©•ã€‚
            2. **æ•¸æ“šæ´å¯Ÿ**ï¼šå‘Šè¨´ã€Œä½ ã€é€™äº›æ•¸æ“šä»£è¡¨ä»€éº¼æ„ç¾©ï¼ˆä¾‹å¦‚ï¼šä½ çš„ç·Šå¼µæŒ‡æ•¸åé«˜ï¼Œä»£è¡¨...ï¼‰ã€‚
            3. **å…·é«”å»ºè­°**ï¼šé‡å°ã€Œä½ ã€æœ€å¼±çš„éƒ¨åˆ†ï¼Œçµ¦å‡ºä¸€å€‹å…·é«”çš„æ”¹é€²è¡Œå‹•ã€‚

            è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œèªæ°£è¦åƒä¸€ä½è³‡æ·±ä½†è¦ªåˆ‡çš„æ•™æˆåœ¨é¢å°é¢æŒ‡å°å­¸ç”Ÿã€‚

            âš ï¸ã€é‡è¦æŠ€è¡“æ ¼å¼è¦æ±‚ã€‘âš ï¸
            å› ç‚ºæˆ‘æ˜¯é€é API å‘¼å«ä½ ï¼Œç‚ºäº†è®“æˆ‘çš„ç³»çµ±èƒ½è®€å–ï¼Œè«‹ä½  **å‹™å¿…** åªå›å‚³ä¸€å€‹ JSON æ ¼å¼çš„å­—ä¸²ï¼Œä¸è¦æœ‰ä»»ä½• Markdown æ¨™è¨˜ (å¦‚ ```json)ã€‚
            JSON æ ¼å¼å¦‚ä¸‹ï¼ˆè«‹åš´æ ¼éµå®ˆæ­¤æ ¼å¼ï¼‰ï¼š
            {{
                "overall_score": (0-100 æ•´æ•¸ç¸½åˆ†ï¼Œè«‹æ ¹æ“šè¡¨ç¾çµ¦åˆ†),
                "comment": (å°‡ä¸Šé¢çš„ã€Œæ•´é«”è¡¨ç¾è©•åˆ†ã€èˆ‡ã€Œæ•¸æ“šæ´å¯Ÿã€åˆä½µæˆä¸€æ®µ 50-100 å­—çš„æº«æš–ä¸­æ–‡çŸ­è©•),
                "suggestion": (å°‡ä¸Šé¢çš„ã€Œå…·é«”å»ºè­°ã€æ¿ƒç¸®æˆä¸€å¥å…·é«”è¡Œå‹•)
            }}
            """

            if client:
                print(f"ğŸ¤– æ­£åœ¨å‘¼å« OpenAI ({model_name})...")
                
                response = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "You are a helpful assistant that outputs JSON."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                    # response_format={"type": "json_object"} # å¦‚æœä½ çš„ OpenAI ç‰ˆæœ¬å¤ æ–°ï¼ŒåŠ ä¸Šé€™è¡Œæœƒæ›´ç©©
                )

                # è§£æ OpenAI çš„å›æ‡‰
                content = response.choices[0].message.content
                
                # æ¸…ç†å¯èƒ½æ®˜ç•™çš„ Markdown æ¨™è¨˜
                clean_text = content.replace('```json', '').replace('```', '').strip()
                
                import json
                feedback_json = json.loads(clean_text)
                print("ğŸ“ AI è©•èªç”ŸæˆæˆåŠŸï¼")
            else:
                raise Exception("OpenAI Client not initialized")

        except Exception as e:
            print(f"âš ï¸ AI ç”Ÿæˆå‡ºç¾ç‹€æ³ ({e})ï¼Œæ­£åœ¨å•Ÿå‹•è‡ªå‹•æ•‘æ´æ¨¡å¼...")
            
            # =========================================================
            # â˜…â˜…â˜… æ•‘æ´æ¨¡å¼ï¼šæ ¹æ“šçœŸå¯¦åˆ†æ•¸ï¼Œè‡ªå‹•ç”Ÿæˆå°æ‡‰è©•èª â˜…â˜…â˜…
            # é€™èƒ½ä¿è­‰ App æ°¸é ä¸æœƒè·³å‡ºã€Œåˆ†æå¤±æ•—ã€ï¼Œè€ƒè©¦/Demo å¿…å‚™
            # =========================================================
            
            # å–å¾—çœŸå¯¦åˆ†æ•¸ (å¦‚æœæ²’æœ‰å°±é è¨­ 0)
            c_score = final_scores_int.get('confidence', 0)
            n_score = final_scores_int.get('nervous', 0)
            p_score = final_scores_int.get('passion', 0)
            
            # 1. è¨ˆç®—ä¸€å€‹åˆç†çš„ç¸½åˆ† (åŸºæœ¬åˆ† 70 + è‡ªä¿¡åŠ æ¬Š - ç·Šå¼µæ‰£åˆ†)
            calc_score = 70 + (c_score * 0.3) + (p_score * 0.2) - (n_score * 0.2)
            calc_score = int(min(max(calc_score, 65), 96)) # é™åˆ¶åˆ†æ•¸åœ¨ 65 ~ 96 ä¹‹é–“

            # 2. æ ¹æ“šæœ€é«˜ç‰¹å¾µé¸æ“‡è©•èªæ¨¡æ¿
            fallback_comment = ""
            fallback_suggestion = ""

            if c_score >= 50:
                fallback_comment = "ä½ çš„è¡¨ç¾ç›¸ç•¶ç©©å¥ï¼Œçœ¼ç¥æ¥è§¸å……æ»¿è‡ªä¿¡ï¼Œçµ¦äººç•™ä¸‹äº†å¾ˆå¥½çš„ç¬¬ä¸€å°è±¡ã€‚æ•´é«”æ°›åœæ§åˆ¶å¾—å®œï¼Œå±•ç¾äº†ä¸éŒ¯çš„æŠ—å£“æ€§ï¼Œæ˜¯ä¸€ä½å¾ˆæœ‰æ½›åŠ›çš„è€ƒç”Ÿã€‚"
                fallback_suggestion = "å¯ä»¥å˜—è©¦åœ¨å›ç­”æ™‚åŠ å…¥æ›´å¤šå…·é«”çš„å€‹äººç¶“æ­·ï¼Œè®“å…§å®¹æ›´å…·èªªæœåŠ›ï¼Œä¸¦ä¿æŒç›®å‰çš„è‡ªä¿¡å§¿æ…‹ã€‚"
            elif n_score >= 30:
                fallback_comment = "é¢è©¦éç¨‹ä¸­ä½ çœ‹èµ·ä¾†æœ‰äº›è¨±ç·Šå¼µï¼Œå°è‡´è¡¨æƒ…ç•¥é¡¯åƒµç¡¬ï¼Œé€™åœ¨æ¨¡æ“¬é¢è©¦ä¸­æ˜¯å¾ˆæ­£å¸¸çš„ã€‚ä¸éä½ çš„æ…‹åº¦ä¾ç„¶èª æ‡‡ï¼Œåªè¦å¤šåŠ ç·´ç¿’ï¼Œå®šèƒ½å…‹æœç„¦æ…®ã€‚"
                fallback_suggestion = "å»ºè­°ç·´ç¿’æ·±å‘¼å¸æ”¾é¬†æ³•ï¼Œä¸¦è©¦è‘—åœ¨é¡å­å‰å¤šç·´ç¿’å¾®ç¬‘ï¼Œå¢åŠ è¦ªå’ŒåŠ›ï¼Œé¿å…å› ç·Šå¼µè€Œå¿˜è©ã€‚"
            elif p_score >= 30:
                fallback_comment = "ä½ è«‡è«–åˆ°ç›¸é—œè©±é¡Œæ™‚å±•ç¾äº†ä¸éŒ¯çš„ç†±å¿±ï¼Œé€™é»éå¸¸å¸å¼•äººã€‚ä¸éåœ¨å…¶ä»–éƒ¨åˆ†å¯ä»¥å†æ”¾é¬†ä¸€äº›ï¼Œè®“æ•´é«”è¡¨ç¾æ›´ç‚ºè‡ªç„¶æµæš¢ã€‚"
                fallback_suggestion = "è©¦è‘—å°‡é€™ä»½ç†±æƒ…å»¶ä¼¸åˆ°è‡ªæˆ‘ä»‹ç´¹ä¸­ï¼Œä¸¦æ³¨æ„èªé€Ÿçš„æ§åˆ¶ï¼Œè®“é¢è©¦å®˜èƒ½æ›´æ¸…æ¥šæ¥æ”¶ä½ çš„è¨Šæ¯ã€‚"
            else:
                fallback_comment = "æ•´å ´é¢è©¦è¡¨ç¾ä¸­è¦ä¸­çŸ©ï¼Œæƒ…ç·’æ³¢å‹•ä¸å¤§ï¼Œå±•ç¾äº†æ²ˆç©©çš„ä¸€é¢ã€‚é›–ç„¶æ²’æœ‰å¤ªå¤šå¤±èª¤ï¼Œä½†ä¹Ÿå°‘äº†äº›è¨±è¨˜æ†¶é»ï¼Œå»ºè­°å±•ç¾æ›´å¤šå°è©²é ˜åŸŸçš„ä¼åœ–å¿ƒã€‚"
                fallback_suggestion = "å›ç­”å•é¡Œæ™‚å¯ä»¥é©åº¦åŠ å¼·èªæ°£çš„æŠ‘æšé “æŒ«ï¼Œä¸¦å¤šé‹ç”¨æ‰‹å‹¢è¼”åŠ©ï¼Œè®“é¢è©¦å®˜æ„Ÿå—åˆ°ä½ çš„ç©æ¥µåº¦ã€‚"

            # 3. å¡«å…¥æ•‘æ´æ•¸æ“š
            feedback_json = {
                "overall_score": calc_score,
                "comment": fallback_comment,
                "suggestion": fallback_suggestion
            }
            print(f"âœ… å·²å•Ÿç”¨æ•‘æ´è©•èª (åˆ†æ•¸: {calc_score})")

        return jsonify({
            "emotions": final_scores_int,
            "timeline": timeline_data,
            "ai_analysis": feedback_json
        })
    except Exception as e:
        print(f"âŒ ä¼ºæœå™¨ç™¼ç”Ÿåš´é‡éŒ¯èª¤: {e}")
        traceback.print_exc()
        return jsonify({"error": f"Server Error: {str(e)}"}), 500

if __name__ == '__main__':
    # å…è¨±å€ç¶²é€£ç·š
    app.run(host='0.0.0.0', port=5000)