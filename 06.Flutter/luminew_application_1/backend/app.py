# fileName: app.py
import os
import cv2
import torch
import torch.nn as nn
from torchvision import transforms, models
from PIL import Image
from flask import Flask, request, jsonify
import google.generativeai as genai
import sys

app = Flask(__name__)

# -----------------------------
# 1. è¨­å®šå°ˆæ¡ˆè·¯å¾‘èˆ‡ API Key
# -----------------------------
PROJECT_DIR = r"C:\MicroExpressionProject"

# â˜… è«‹ç¢ºèªæ‚¨çš„æ¨¡å‹æª”åæ˜¯å¦æ­£ç¢º
MODEL_PATH = os.path.join(PROJECT_DIR, "models", "test_best_.pth") 
HAAR_CASCADE_PATH = os.path.join(PROJECT_DIR, "data", "haarcascade_frontalface_default.xml")

# â˜…â˜…â˜… æ‚¨çš„ GOOGLE API Key â˜…â˜…â˜…
GOOGLE_API_KEY = "AIzaSyD6795y_wZdy-3nyioKwTS5OHFj4uIvIOs"

# è¨­å®š Google API
try:
    genai.configure(api_key=GOOGLE_API_KEY)
except Exception as e:
    print(f"âš ï¸ Google API è¨­å®šå¤±æ•—ï¼Œè«‹æª¢æŸ¥ Key: {e}")

# -----------------------------
# 2. æª¢æŸ¥æª”æ¡ˆèˆ‡è¼‰å…¥æ¨¡å‹
# -----------------------------
if not os.path.exists(MODEL_PATH):
    print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”ï¼š{MODEL_PATH}")
    sys.exit(1)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"â–¶ æ­£åœ¨è¼‰å…¥æ¨¡å‹... (ä½¿ç”¨è¨­å‚™: {device})")

checkpoint = torch.load(MODEL_PATH, map_location=device)
state_dict = checkpoint["state_dict"] if "state_dict" in checkpoint else checkpoint

# å®šç¾©é¡åˆ¥
CLASSES = ['confidence', 'nervous', 'passion', 'relaxed'] 

model = models.resnet18(pretrained=False)
fc_keys = [k for k in state_dict.keys() if k.startswith("fc.")]
use_sequential = any(k.startswith("fc.1.") for k in fc_keys)

if use_sequential:
    model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, len(CLASSES)))
else:
    model.fc = nn.Linear(model.fc.in_features, len(CLASSES))
    
try:
    model.load_state_dict(state_dict)
except:
    model.load_state_dict(state_dict, strict=False)
    
model = model.to(device)
model.eval()

# è¼‰å…¥äººè‡‰è¾¨è­˜
if not os.path.exists(HAAR_CASCADE_PATH):
    HAAR_CASCADE_PATH = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
print(f"Face Cascade Path: {HAAR_CASCADE_PATH}")
face_cascade = cv2.CascadeClassifier(HAAR_CASCADE_PATH)

# å½±åƒé è™•ç†
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

# -----------------------------
# 3. è™•ç† API è«‹æ±‚ (æ‰‹æ©Ÿä¸Šå‚³å½±ç‰‡)
# -----------------------------
@app.route('/analyze', methods=['POST'])
def analyze_video():
    if 'video' not in request.files:
        return jsonify({"error": "No video file provided"}), 400
    
    video_file = request.files['video']
    save_path = os.path.join(PROJECT_DIR, "temp_upload.mp4")
    video_file.save(save_path)
    
    print("ğŸ“¥ æ”¶åˆ°å½±ç‰‡ï¼Œé–‹å§‹åˆ†æ...")
    cap = cv2.VideoCapture(save_path)
    session_history = []
    
    frame_count = 0
    detected_count = 0

    # ç”¨ä¾†è¨˜ä½æœ€ä½³çš„æ—‹è½‰è§’åº¦ï¼Œä¹‹å¾Œçš„æ¯ä¸€å¹€å°±ä¸ç”¨ä¸€ç›´è©¦äº†ï¼ŒåŠ é€Ÿé‹ç®—
    best_rotation = None 

    with torch.no_grad():
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_count += 1
            if frame_count % 3 != 0: continue # åŠ é€Ÿï¼šæ¯3å¹€åªæ¸¬1å¹€

            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            faces = []
            
            # --- è¶…å¼·è‡ªå‹•æ—‹è½‰åµæ¸¬é‚è¼¯ ---
            
            # 1. å¦‚æœå·²ç¶“çŸ¥é“æœ€ä½³è§’åº¦ï¼Œç›´æ¥è½‰
            if best_rotation is not None:
                frame_rotated = cv2.rotate(frame, best_rotation)
                gray_rotated = cv2.cvtColor(frame_rotated, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray_rotated, 1.1, 5)
                if len(faces) > 0:
                    frame = frame_rotated # æ›´æ–°ç•«é¢

            # 2. å¦‚æœé‚„ä¸çŸ¥é“ï¼Œæˆ–æ˜¯è½‰äº†ä¹‹å¾Œçªç„¶æ‰¾ä¸åˆ°ï¼Œå°±æš´åŠ›å˜—è©¦æ‰€æœ‰è§’åº¦
            if len(faces) == 0:
                # å®šç¾©è¦å˜—è©¦çš„æ—‹è½‰ä»£ç¢¼ï¼š[åŸå§‹(è·³é), 90åº¦, 270åº¦, 180åº¦]
                rotations = [
                    (None, "åŸå§‹"),
                    (cv2.ROTATE_90_CLOCKWISE, "90åº¦"),
                    (cv2.ROTATE_90_COUNTERCLOCKWISE, "270åº¦"),
                    (cv2.ROTATE_180, "180åº¦")
                ]
                
                for code, name in rotations:
                    if code is None:
                        # è©¦åŸå§‹
                        check_frame = frame
                        check_gray = gray
                    else:
                        # è©¦æ—‹è½‰
                        check_frame = cv2.rotate(frame, code)
                        check_gray = cv2.cvtColor(check_frame, cv2.COLOR_BGR2GRAY)
                    
                    found_faces = face_cascade.detectMultiScale(check_gray, 1.1, 5)
                    
                    if len(found_faces) > 0:
                        faces = found_faces
                        frame = check_frame
                        if code is not None:
                            best_rotation = code # è¨˜ä½é€™å€‹è§’åº¦ï¼
                            # print(f"ğŸ’¡ é–å®šæ—‹è½‰è§’åº¦: {name}")
                        break # æ‰¾åˆ°äº†å°±è·³å‡ºè¿´åœˆ

            if len(faces) == 0:
                continue 

            detected_count += 1
            
            # å–æœ€å¤§çš„äººè‡‰
            (x, y, w, h) = max(faces, key=lambda f: f[2] * f[3])
            face_crop = frame[y:y+h, x:x+w]
            
            try:
                img = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)
                img = Image.fromarray(img)
                img_tensor = transform(img).unsqueeze(0).to(device)

                outputs = model(img_tensor)
                probs = torch.softmax(outputs, dim=1)[0]

                current_emotions = {}
                for i, cls in enumerate(CLASSES):
                    current_emotions[cls] = probs[i].item()
                session_history.append(current_emotions)
            except Exception as e:
                pass

    cap.release()
    print(f"ğŸ“Š åˆ†æå®Œæˆï¼šå…±è®€å– {frame_count} å¹€ï¼ŒæˆåŠŸè¾¨è­˜ {detected_count} å¹€äººè‡‰ã€‚")
    
    # è¨ˆç®—è¾¨è­˜ç‡
    if frame_count > 0:
        rate = (detected_count / (frame_count/3)) * 100 # é™¤ä»¥3æ˜¯å› ç‚ºæˆ‘å€‘æœ‰è·³å¹€
        print(f"ğŸ¯ è¾¨è­˜ç‡ç´„: {rate:.1f}%")

    if not session_history:
        return jsonify({"error": "No face detected (è«‹è©¦è‘—æ‹¿é ä¸€é»æˆ–ç¢ºèªå…‰ç·š)"}), 400

    # è¨ˆç®—åˆ†æ•¸
    avg_scores = {cls: 0.0 for cls in CLASSES}
    for entry in session_history:
        for cls in CLASSES:
            avg_scores[cls] += entry[cls]
            
    final_scores_float = {}
    for cls in CLASSES:
        final_scores_float[cls] = (avg_scores[cls] / len(session_history)) * 100
    
    final_scores_int = {k: int(v) for k, v in final_scores_float.items()}

    print("-" * 40)
    print(f"ğŸ“ˆ æƒ…ç·’åˆ†ä½ˆçµ±è¨ˆ: {final_scores_float}")
    print("-" * 40)

    # -----------------------------
    # 4. ç”Ÿæˆ AI è©•èª
    # -----------------------------
    feedback_json = {
        "overall_score": 0, 
        "comment": "AI åˆ†æå¤±æ•—", 
        "suggestion": "è«‹ç¨å¾Œå†è©¦"
    }
    
    try:
        print("ğŸ¤– æ­£åœ¨å‘¼å« Google Gemini AI é¢è©¦å®˜...")
        
        prompt = f"""
        ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¤§å­¸å…¥å­¸é¢è©¦æ•™ç·´ã€‚ä½ å‰›å‰›è§€å¯Ÿäº†ä¸€ä½é«˜ä¸­ç”Ÿçš„æ¨¡æ“¬é¢è©¦è¡¨ç¾ã€‚
        ä»¥ä¸‹æ˜¯é€é AI å¾®è¡¨æƒ…åˆ†æç³»çµ±åµæ¸¬åˆ°çš„æƒ…ç·’æ•¸æ“šï¼ˆæ•´å ´é¢è©¦çš„å¹³å‡ä½”æ¯”ï¼‰ï¼š

        ã€æƒ…ç·’æ•¸æ“šã€‘
        - Confidence (è‡ªä¿¡): {final_scores_float.get('confidence', 0):.1f}%
        - Passion (ç†±å¿±): {final_scores_float.get('passion', 0):.1f}%
        - Relaxed (æ²ˆç©©/åŸºæº–ç·š): {final_scores_float.get('relaxed', 0):.1f}%
        - Nervous (ç·Šå¼µ/ç„¦æ…®): {final_scores_float.get('nervous', 0):.1f}%

        ã€æƒ…ç·’å®šç¾©åƒè€ƒã€‘
        1. Confidence: çœ¼ç¥å …å®šã€æœ‰è‡ªä¿¡ã€‚
        2. Passion: è«‡è«–èˆˆè¶£æ™‚å±•ç¾çš„ç†±æƒ…ã€‚
        3. Relaxed: å°ˆæ³¨è†è½æˆ–æƒ…ç·’å¹³ç©©ï¼ˆåŸºæº–ç·šï¼‰ã€‚
        4. Nervous: ç„¦æ…®ã€åƒµç¡¬æˆ–ä¸è‡ªç„¶ã€‚

        ã€ä»»å‹™ã€‘
        è«‹æ ¹æ“šä»¥ä¸Šæ•¸æ“šï¼Œç›´æ¥å°è‘—é€™ä½è€ƒç”Ÿï¼ˆä½¿ç”¨ã€Œä½ ã€ä¾†ç¨±å‘¼ï¼‰ï¼Œç”Ÿæˆä¸€ä»½ç°¡çŸ­æœ‰åŠ›çš„ã€Œé¢è©¦è¡¨ç¾åˆ†æå ±å‘Šã€ã€‚
        
        âš ï¸ã€é‡è¦æ ¼å¼è¦æ±‚ã€‘âš ï¸
        å› ç‚ºæˆ‘æ˜¯é€é API å‘¼å«ä½ ï¼Œè«‹ä½  **å‹™å¿…** åªå›å‚³ä¸€å€‹ JSON æ ¼å¼çš„å­—ä¸²ï¼Œä¸è¦æœ‰ä»»ä½• Markdown æ¨™è¨˜ (å¦‚ ```json)ã€‚
        JSON æ ¼å¼å¦‚ä¸‹ï¼š
        {{
            "overall_score": (æ ¹æ“šè‡ªä¿¡èˆ‡ç†±å¿±æ¯”ä¾‹çµ¦å‡ºçš„ 0-100 æ•´æ•¸ç¸½åˆ†),
            "comment": (é‡å°æ•´é«”è¡¨ç¾èˆ‡æ•¸æ“šæ´å¯Ÿçš„ä¸€æ®µè©±ï¼Œèªæ°£è¦åƒè³‡æ·±è¦ªåˆ‡çš„æ•™æˆ),
            "suggestion": (é‡å°æœ€å¼±éƒ¨åˆ†çµ¦å‡ºçš„å…·é«”æ”¹é€²è¡Œå‹•)
        }}
        """

        model_gen = genai.GenerativeModel('gemini-2.0-flash')
        response = model_gen.generate_content(prompt)
        
        clean_text = response.text.replace('```json', '').replace('```', '').strip()
        import json
        feedback_json = json.loads(clean_text)
        
        print("\nğŸ“ AI å›æ‡‰æˆåŠŸï¼")
        
    except Exception as e:
        print(f"âŒ AI ç”Ÿæˆå¤±æ•—: {e}")

    return jsonify({
        "emotions": final_scores_int,
        "ai_analysis": feedback_json
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)